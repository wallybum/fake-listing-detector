import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.action_chains import ActionChains
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import pandas as pd
import time
import os
import random
from datetime import datetime, timedelta, timezone
from supabase import create_client, Client
from pyvirtualdisplay import Display 

# ==================================================================
# [ì„¤ì •] í™˜ê²½ë³€ìˆ˜
# ==================================================================
SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_KEY")
COMPLEX_NO = "108064"

if not SUPABASE_URL or not SUPABASE_KEY:
    print("âŒ Supabase ì„¤ì •ì´ ì—†ìŠµë‹ˆë‹¤.")
    exit()

supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

KST = timezone(timedelta(hours=9))
NOW = datetime.now(KST)
TODAY_STR = NOW.strftime("%Y-%m-%d")
HOUR_STR = NOW.strftime("%H")

def run_crawler():
    print(f"ğŸš€ [GitHub Actions] {TODAY_STR} {HOUR_STR}ì‹œ í¬ë¡¤ë§ ì‹œì‘...")

    display = Display(visible=0, size=(1920, 1080))
    display.start()
    
    options = uc.ChromeOptions()
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--window-size=1920,1080")
    options.add_argument("--lang=ko_KR")
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")

    # ë²„ì „ ê³ ì • (GitHub Actions í™˜ê²½ ëŒ€ì‘)
    driver = uc.Chrome(options=options, version_main=142)
    
    driver.execute_cdp_cmd("Page.addScriptToEvaluateOnNewDocument", {
        "source": """
            Object.defineProperty(navigator, 'webdriver', { get: () => undefined });
        """
    })
    
    try:
        driver.get(f"https://new.land.naver.com/complexes/{COMPLEX_NO}")
        
        try: WebDriverWait(driver, 40).until(EC.presence_of_element_located((By.ID, "complex_article_trad_type_filter_0")))
        except: pass

        # ------------------------------------------------------------------
        # 2. í•„í„° ì„¤ì •
        # ------------------------------------------------------------------
        print("âš™ï¸ í•„í„° ì ìš© ì¤‘...")
        try:
            driver.execute_script("if(document.querySelector('#complex_article_trad_type_filter_0:checked')) document.querySelector('#complex_article_trad_type_filter_0').click();")
            time.sleep(0.5)
            driver.execute_script("if(!document.querySelector('#complex_article_trad_type_filter_1:checked')) document.querySelector('#complex_article_trad_type_filter_1').click();")
            time.sleep(1)
            
            group_input = driver.find_element(By.ID, "address_group2")
            if not group_input.is_selected():
                driver.execute_script("arguments[0].click();", driver.find_element(By.CSS_SELECTOR, "label[for='address_group2']"))
                time.sleep(1)
            
            driver.find_element(By.CSS_SELECTOR, "a.sorting_type[data-nclk='TAA.price']").click()
            
            print("   â³ ëª©ë¡ ê°±ì‹  ëŒ€ê¸° (5ì´ˆ)...")
            time.sleep(5)

        except Exception as e:
            print(f"âš ï¸ í•„í„° ì˜¤ë¥˜: {e}")
        
        # ------------------------------------------------------------------
        # 3. ìŠ¤í¬ë¡¤ ë¡œì§
        # ------------------------------------------------------------------
        print("â¬‡ï¸ ë°ì´í„° ë¡œë”© ì¤‘ (ì „ì²´ ë§¤ë¬¼ í™•ë³´)...")
        
        try: list_area = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, "articleListArea")))
        except: list_area = driver.find_element(By.TAG_NAME, "body")

        try: 
            actions = ActionChains(driver)
            actions.move_to_element(list_area).click().perform()
        except: pass

        last_count = 0
        same_count_loop = 0
        
        for _ in range(50):
            items = driver.find_elements(By.CSS_SELECTOR, "div.item:not(.item--child)")
            curr_count = len(items)
            
            print(f"   ... ìŠ¤í¬ë¡¤ ì¤‘ (í˜„ì¬ {curr_count}ê°œ)")
            
            if curr_count > 0:
                last_item = items[-1]
                driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", last_item)
            
            driver.execute_script("arguments[0].scrollTop = arguments[0].scrollHeight", list_area)
            try: list_area.send_keys(Keys.PAGE_DOWN)
            except: pass

            time.sleep(2.0)
            
            if curr_count == last_count and curr_count > 0:
                same_count_loop += 1
                if same_count_loop >= 5:
                    print(f"   âœ… ì „ì²´ ëª©ë¡ ë¡œë”© ì™„ë£Œ (ìµœì¢… {curr_count}ê°œ ê·¸ë£¹)")
                    break
            else:
                same_count_loop = 0
                
            last_count = curr_count

        # ------------------------------------------------------------------
        # 4. ë°ì´í„° ì¶”ì¶œ (ë§¤ë¬¼ë²ˆí˜¸ ë¡œì§ ì¶”ê°€ë¨)
        # ------------------------------------------------------------------
        parent_items = driver.find_elements(By.CSS_SELECTOR, "div.item:not(.item--child)")
        print(f"ğŸ“ ì´ {len(parent_items)}ê°œ ê·¸ë£¹ ë°œê²¬.")

        if len(parent_items) == 0:
            print("âŒ ë°ì´í„° 0ê±´.")
            driver.save_screenshot("debug_zero.png")
            return

        db_data = []
        
        for idx, parent in enumerate(parent_items):
            try:
                p_html = parent.get_attribute('outerHTML')
                soup = BeautifulSoup(p_html, "html.parser")
                try: title = soup.select_one("div.item_title > span.text").get_text(strip=True)
                except: continue
                if title == "ì œëª©ì—†ìŒ": continue
                
                dong = title.replace("DMCíŒŒí¬ë·°ìì´", "").strip()
                try: spec = soup.select_one("div.info_area .spec").get_text(strip=True)
                except: spec = ""

                # í¼ì¹˜ê¸° ë¡œì§ 
                multi_btn = parent.find_elements(By.CSS_SELECTOR, "span.label--multicp")
                targets = []
              
                # (ì¤‘ê°œì‚¬ Nê³³ ë²„íŠ¼) ë²„íŠ¼ì´ ìˆì„ ê²½ìš°
                if multi_btn:
                    driver.execute_script("arguments[0].click();", multi_btn[0])
                    time.sleep(0.3)
                    driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", parent)
                    
                    child_container = parent.find_element(By.CSS_SELECTOR, "div.item.item--child")
                    inners = child_container.find_elements(By.CSS_SELECTOR, "div.item_inner")
                    for inner in inners:
                        if inner.find_elements(By.CSS_SELECTOR, "div.cp_area"): targets.append(inner)
                else:
                    targets.append(parent.find_element(By.CSS_SELECTOR, "div.item_inner"))

                # ----------------------------------------------------------
                # [ìˆ˜ì •] ë§¤ë¬¼ë²ˆí˜¸(article_no) ì¶”ì¶œ ë¡œì§ ì ìš©
                # ----------------------------------------------------------
                for target in targets:
                    # ğŸŒŸ [í•„ìˆ˜] ë£¨í”„ ì‹œì‘í•  ë•Œë§ˆë‹¤ ë³€ìˆ˜ ì´ˆê¸°í™” (ì´ì „ ê°’ ë®ì–´ì“°ê¸° ë°©ì§€)
                    article_no = None
                    agent_name = None
                    price = ""

                    try:
                        # ----------------------------------------------------------
                        # 1. í´ë¦­í•  ìš”ì†Œ ê²°ì • (ì‚¬ìš©ì ìš”ì²­ ë¡œì§ ë°˜ì˜)
                        # ----------------------------------------------------------
                        click_element = None
                        is_naver_view = False # ë””ë²„ê¹…ìš© í”Œë˜ê·¸

                        # label_area ì•ˆì˜ "ë„¤ì´ë²„ì—ì„œ ë³´ê¸°(label--cp)" ë²„íŠ¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
                        # find_elements(ë³µìˆ˜í˜•)ë¥¼ ì“°ë©´ ì—†ì–´ë„ ì—ëŸ¬ê°€ ì•ˆ ë‚˜ê³  ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
                        naver_btns = target.find_elements(By.CSS_SELECTOR, "div.label_area a.label--cp")

                        if len(naver_btns) > 0:
                            # [Case A] ë²„íŠ¼ì´ ìˆìŒ -> ë²„íŠ¼ì„ í´ë¦­ íƒ€ê²Ÿìœ¼ë¡œ ì„¤ì •
                            click_element = naver_btns[0]
                            is_naver_view = True
                            # print("   ğŸ‘‰ [Button] 'ë„¤ì´ë²„ì—ì„œ ë³´ê¸°' í´ë¦­")
                        else:
                            # [Case B] ë²„íŠ¼ì´ ì—†ìŒ -> ì¼ë°˜ ì œëª© ë§í¬ë¥¼ í´ë¦­ íƒ€ê²Ÿìœ¼ë¡œ ì„¤ì •
                            click_element = target.find_element(By.CSS_SELECTOR, "a.item_link")
                            is_naver_view = False

                        # ----------------------------------------------------------
                        # 2. í´ë¦­ ì‹¤í–‰ & ìƒì„¸ íŒ¨ë„ ë¡œë”©
                        # ----------------------------------------------------------
                        driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", target)
                        driver.execute_script("arguments[0].click();", click_element)
                        
                        time.sleep(0.6) # íŒ¨ë„ ì—´ë¦¬ëŠ” ì‹œê°„ í™•ë³´

                        # ìš°ì¸¡ ìƒì„¸ íŒ¨ë„ì´ ëœ° ë•Œê¹Œì§€ ëŒ€ê¸°
                        try:
                            WebDriverWait(driver, 2).until(
                                EC.presence_of_element_located((By.CSS_SELECTOR, "div.detail_contents_inner"))
                            )
                        except:
                            pass 

                        # ----------------------------------------------------------
                        # 3. ìƒì„¸ íŒ¨ë„ íŒŒì‹± (ì—¬ê¸°ê°€ ë©”ì¸)
                        # ----------------------------------------------------------
                        full_soup = BeautifulSoup(driver.page_source, "html.parser")
                        detail_area = full_soup.select_one("div.detail_contents_inner")

                        if detail_area:
                            rows = detail_area.select("tr.info_table_item")
                            for row in rows:
                                th = row.select_one("th")
                                # 'ë§¤ë¬¼ë²ˆí˜¸'ë¼ê³  ì íŒ í–‰ì„ ì°¾ì•„ì„œ ê·¸ ì˜†ì˜ td ê°’ì„ ê°€ì ¸ì˜´
                                if th and "ë§¤ë¬¼ë²ˆí˜¸" in th.get_text():
                                    td = row.select_one("td")
                                    if td:
                                        article_no = td.get_text(strip=True)
                                        break
                        
                        # [ë³´ì™„] ë§Œì•½ ìƒì„¸ íŒ¨ë„ ë¡œë”©ì´ ë„ˆë¬´ ëŠë ¤ì„œ ì‹¤íŒ¨í–ˆê±°ë‚˜ íŒŒì‹± ëª»í–ˆì„ ê²½ìš°
                        # ì¼ë°˜ ë§¤ë¬¼(Case B)ì´ë¼ë©´ ë¦¬ìŠ¤íŠ¸ì— ìˆëŠ” data-attributeë¼ë„ ê°€ì ¸ì™€ë³¸ë‹¤.
                        # (ë„¤ì´ë²„ì—ì„œ ë³´ê¸° ë§¤ë¬¼ì€ ë¦¬ìŠ¤íŠ¸ì— ë²ˆí˜¸ê°€ ì—†ëŠ” ê²½ìš°ê°€ ë§ìœ¼ë¯€ë¡œ íŒ¨ë„ íŒŒì‹±ì´ í•„ìˆ˜)
                        if not article_no and not is_naver_view:
                            try:
                                article_no = click_element.get_attribute("data-article-no")
                            except: pass

                        # ----------------------------------------------------------
                        # 4. ë‚˜ë¨¸ì§€ ì •ë³´ ì¶”ì¶œ ë° ì €ì¥
                        # ----------------------------------------------------------
                        t_html = target.get_attribute('outerHTML')
                        t_soup = BeautifulSoup(t_html, "html.parser")

                        try: agent_name = t_soup.select("a.agent_name")[-1].get_text(strip=True)
                        except: agent_name = "ì•Œìˆ˜ì—†ìŒ"
                        
                        try: price = t_soup.select_one("span.price").get_text(strip=True)
                        except: price = ""


                        is_landlord = False
                        try:
                            # .icon-badge.type-owner í´ë˜ìŠ¤ë¥¼ ê°€ì§„ íƒœê·¸ ì°¾ê¸°
                            owner_badge = t_soup.select_one(".icon-badge.type-owner")
                            if owner_badge and "ì§‘ì£¼ì¸" in owner_badge.get_text():
                                is_landlord = True
                        except:
                            pass


                        # ğŸŒŸ [ê²€ì¦] ë§¤ë¬¼ë²ˆí˜¸ê°€ ì—¬ì „íˆ Noneì´ë©´ ì €ì¥ ê±´ë„ˆë›°ê¸°
                        if not article_no:
                            print(f"   âŒ ë§¤ë¬¼ë²ˆí˜¸ ì¶”ì¶œ ì‹¤íŒ¨ (Skip) - {agent_name}")
                            continue

                        print(f"   ğŸš€ [ìˆ˜ì§‘] {dong} / {price} / {agent_name} / ë²ˆí˜¸:{article_no}")

                        db_data.append({
                            "agent": agent_name, "dong": dong, "spec": spec, "price": price,
                            "article_no": article_no, "trade_type": "ë§¤ë§¤", 
                            "crawl_date": TODAY_STR, "crawl_time": f"{HOUR_STR}ì‹œ",
                            "is_landlord": is_landlord
                        })

                    except Exception as e:
                        print(f"   âŒ íŒŒì‹± ì—ëŸ¬: {e}")
                        continue
            except: continue
        
        driver.quit()

        # ------------------------------------------------------------------
        # 5. DB ì €ì¥
        # ------------------------------------------------------------------
        if db_data:
            try:
                supabase.table('real_estate_logs').insert(db_data).execute()
                print(f"âœ… [Log] ì´ {len(db_data)}ê±´ ì €ì¥ ì™„ë£Œ")
            except Exception as e:
                print(f"âŒ [Log] ì €ì¥ ì‹¤íŒ¨: {e}")

            df = pd.DataFrame(db_data)
            stats_df = df['agent'].value_counts().reset_index()
            stats_df.columns = ['agent', 'count']
            
            stats_data = []
            for _, row in stats_df.iterrows():
                stats_data.append({
                    "agent": row['agent'],
                    "count": int(row['count']),
                    "crawl_date": TODAY_STR,
                    "crawl_time": f"{HOUR_STR}ì‹œ"
                })
            
            try:
                supabase.table('agent_stats').insert(stats_data).execute()
                print(f"âœ… [Stats] í†µê³„ ì €ì¥ ì™„ë£Œ")
            except Exception as e:
                print(f"âŒ [Stats] ì €ì¥ ì‹¤íŒ¨: {e}")

    except Exception as e:
        print(f"âŒ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        driver.save_screenshot("debug_fatal.png")
        driver.quit()
    finally:
        display.stop()

if __name__ == "__main__":
    run_crawler()