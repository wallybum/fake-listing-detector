import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.action_chains import ActionChains
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import pandas as pd
from bs4 import BeautifulSoup
import time
import os
import random
from datetime import datetime, timedelta, timezone
from supabase import create_client, Client

# â–¼â–¼â–¼ [í•„ìˆ˜] ê°€ìƒ ëª¨ë‹ˆí„° ë¼ì´ë¸ŒëŸ¬ë¦¬ (ì´ê²Œ ìˆì–´ì•¼ ì„œë²„ì—ì„œ 0ê±´ì´ ì•ˆ ëœ¸) â–¼â–¼â–¼
from pyvirtualdisplay import Display

# ==================================================================
# [ì„¤ì •] í™˜ê²½ë³€ìˆ˜
# ==================================================================
SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_KEY")
COMPLEX_NO = "108064"

if not SUPABASE_URL or not SUPABASE_KEY:
    print("âŒ Supabase ì„¤ì •ì´ ì—†ìŠµë‹ˆë‹¤.")
    exit()

supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

# í•œêµ­ ì‹œê°„ ì„¤ì •
KST = timezone(timedelta(hours=9))
NOW = datetime.now(KST)
TODAY_STR = NOW.strftime("%Y-%m-%d")
HOUR_STR = NOW.strftime("%H")

def run_crawler():
    print(f"ğŸš€ [GitHub Actions + ê°€ìƒí™”ë©´] {TODAY_STR} {HOUR_STR}ì‹œ í¬ë¡¤ë§ ì‹œì‘...")

    # 1. ê°€ìƒ ëª¨ë‹ˆí„° ì¼œê¸° (ì„œë²„ì—ì„œë„ í™”ë©´ì´ ìˆëŠ” ì²™ ì†ì„)
    display = Display(visible=0, size=(1920, 1080))
    display.start()
    
    options = uc.ChromeOptions()
    # [ì¤‘ìš”] --headless ì˜µì…˜ ì‚­ì œ! (ê°€ìƒ í™”ë©´ì„ ì“°ë¯€ë¡œ í•„ìš” ì—†ìŒ)
    
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--window-size=1920,1080")
    options.add_argument("--lang=ko_KR")
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")

    driver = uc.Chrome(options=options)

    # ë´‡ íƒì§€ ì†ì„± ì œê±°
    driver.execute_cdp_cmd("Page.addScriptToEvaluateOnNewDocument", {
        "source": """
            Object.defineProperty(navigator, 'webdriver', { get: () => undefined });
        """
    })
    
    try:
        # 2. í˜ì´ì§€ ì ‘ì†
        driver.get(f"https://new.land.naver.com/complexes/{COMPLEX_NO}")
        
        # ë¡œë”© ëŒ€ê¸° (ìµœëŒ€ 60ì´ˆ)
        try: 
            WebDriverWait(driver, 60).until(
                EC.presence_of_element_located((By.ID, "complex_article_trad_type_filter_0"))
            )
        except: 
            print("âš ï¸ ë¡œë”© ì‹œê°„ ì´ˆê³¼ or ì°¨ë‹¨ë¨")
            driver.save_screenshot("debug_loading_fail.png")

        # 3. [í•„í„° ì„¤ì •] 74ê±´ì„ ë§ì¶”ê¸° ìœ„í•œ ì •ë°€ ë¡œì§
        print("âš™ï¸ í•„í„° ì ìš© ì¤‘...")
        try:
            # (1) ì „ì²´ ì„ íƒ í•´ì œ
            driver.execute_script("if(document.querySelector('#complex_article_trad_type_filter_0:checked')) document.querySelector('#complex_article_trad_type_filter_0').click();")
            time.sleep(0.5)
            
            # (2) ë§¤ë§¤ ì„ íƒ
            driver.execute_script("if(!document.querySelector('#complex_article_trad_type_filter_1:checked')) document.querySelector('#complex_article_trad_type_filter_1').click();")
            time.sleep(1)
            
            # (3) ë™ì¼ë§¤ë¬¼ ë¬¶ê¸° (ì²´í¬ ì•ˆ ë˜ì–´ ìˆìœ¼ë©´ í´ë¦­)
            # ì„œë²„ì—ì„œëŠ” JSë¡œ ê°•ì œ í´ë¦­í•˜ëŠ” ê²ƒì´ ë” í™•ì‹¤í•¨
            driver.execute_script("""
                var chk = document.getElementById("address_group2");
                if (!chk.checked) {
                    document.querySelector("label[for='address_group2']").click();
                }
            """)
            time.sleep(1)
            
            # (4) ê°€ê²©ìˆœ ì •ë ¬
            driver.find_element(By.CSS_SELECTOR, "a.sorting_type[data-nclk='TAA.price']").click()
            
            # [ì¤‘ìš”] í•„í„° ì ìš© í›„ ëª©ë¡ì´ ë°”ë€” ë•Œê¹Œì§€ ì¶©ë¶„íˆ ëŒ€ê¸° (5ì´ˆ)
            print("   â³ ëª©ë¡ ê°±ì‹  ëŒ€ê¸°...")
            time.sleep(5)

        except Exception as e:
            print(f"âš ï¸ í•„í„° ì˜¤ë¥˜(ë¬´ì‹œ): {e}")
        
        # 4. ìŠ¤í¬ë¡¤ ë¡œì§ (ê°œìˆ˜ ì²´í¬ + ê°•ì œ ìŠ¤í¬ë¡¤)
        print("â¬‡ï¸ ë°ì´í„° ë¡œë”© ì¤‘...")
        try: list_area = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, "articleListArea")))
        except: list_area = driver.find_element(By.TAG_NAME, "body")

        # í¬ì»¤ìŠ¤
        try:
            actions = ActionChains(driver)
            actions.move_to_element(list_area).click().perform()
        except: pass

        last_count = 0
        same_count = 0
        
        for _ in range(40): # ìµœëŒ€ 40ë²ˆ ì‹œë„
            driver.execute_script("arguments[0].scrollTop = arguments[0].scrollHeight", list_area)
            try: 
                list_area.send_keys(Keys.END)
                time.sleep(0.2)
                list_area.send_keys(Keys.PAGE_DOWN)
            except: pass
            
            time.sleep(1.5)
            
            # ìì‹ ìš”ì†Œ ì œì™¸í•˜ê³  ë¶€ëª¨ ê·¸ë£¹ë§Œ ì¹´ìš´íŠ¸
            items = driver.find_elements(By.CSS_SELECTOR, "div.item:not(.item--child)")
            curr = len(items)
            print(f"   ... ë¡œë”© ì¤‘ ({curr}ê°œ)")

            if curr == last_count and curr > 0:
                same_count += 1
                if same_count >= 5: 
                    print("   âœ… ìŠ¤í¬ë¡¤ ì™„ë£Œ")
                    break
            else: same_count = 0
            last_count = curr

        # 5. ë°ì´í„° ì¶”ì¶œ
        parent_items = driver.find_elements(By.CSS_SELECTOR, "div.item:not(.item--child)")
        print(f"ğŸ“ ì´ {len(parent_items)}ê°œ ê·¸ë£¹ ë°œê²¬.")

        if len(parent_items) == 0:
            print("âŒ ë°ì´í„° 0ê±´. ì°¨ë‹¨ë˜ì—ˆì„ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.")
            driver.save_screenshot("debug_zero_result.png")
            return

        db_data = []
        
        for parent in parent_items:
            try:
                p_html = parent.get_attribute('outerHTML')
                soup = BeautifulSoup(p_html, "html.parser")
                try: title = soup.select_one("div.item_title > span.text").get_text(strip=True)
                except: continue
                if title == "ì œëª©ì—†ìŒ": continue
                
                dong = title.replace("DMCíŒŒí¬ë·°ìì´", "").strip()
                try: spec = soup.select_one("div.info_area .spec").get_text(strip=True)
                except: spec = ""

                # í¼ì¹˜ê¸° (ìƒì„¸ ì •ë³´)
                multi_btn = parent.find_elements(By.CSS_SELECTOR, "span.label--multicp")
                targets = []
                
                if multi_btn:
                    driver.execute_script("arguments[0].click();", multi_btn[0])
                    time.sleep(0.2)
                    driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", parent)
                    
                    container = parent.find_element(By.CSS_SELECTOR, "div.item.item--child")
                    inners = container.find_elements(By.CSS_SELECTOR, "div.item_inner")
                    for inner in inners:
                        if inner.find_elements(By.CSS_SELECTOR, "div.cp_area"): targets.append(inner)
                else:
                    targets.append(parent.find_element(By.CSS_SELECTOR, "div.item_inner"))

                for target in targets:
                    t_soup = BeautifulSoup(target.get_attribute('outerHTML'), "html.parser")
                    try: agent = t_soup.select("a.agent_name")[-1].get_text(strip=True)
                    except: agent = "ì•Œìˆ˜ì—†ìŒ"
                    try: price = t_soup.select_one("span.price").get_text(strip=True)
                    except: price = ""
                    
                    db_data.append({
                        "agent": agent, "dong": dong, "spec": spec, "price": price,
                        "article_no": "-", # í´ë¦­ ìƒëµ
                        "crawl_date": TODAY_STR, "crawl_time": f"{HOUR_STR}ì‹œ"
                    })
            except: continue
        
        driver.quit()

        # DB ì €ì¥
        if db_data:
            try:
                supabase.table('real_estate_logs').insert(db_data).execute()
                print(f"âœ… [Log] {len(db_data)}ê±´ ì €ì¥ ì„±ê³µ")
            except Exception as e:
                print(f"âŒ [Log] ì €ì¥ ì‹¤íŒ¨: {e}")

            # í†µê³„ ì €ì¥
            import pandas as pd
            df = pd.DataFrame(db_data)
            stats = df['agent'].value_counts().reset_index()
            stats.columns = ['agent', 'count']
            stats_data = [{"agent": r['agent'], "count": int(r['count']), "crawl_date": TODAY_STR, "crawl_time": f"{HOUR_STR}ì‹œ"} for _, r in stats.iterrows()]
            
            try:
                supabase.table('agent_stats').insert(stats_data).execute()
                print(f"âœ… [Stats] í†µê³„ ì €ì¥ ì„±ê³µ")
            except: pass

    except Exception as e:
        print(f"âŒ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        driver.save_screenshot("debug_fatal.png")
        driver.quit()
    finally:
        display.stop() # ê°€ìƒ ëª¨ë‹ˆí„° ì¢…ë£Œ

if __name__ == "__main__":
    run_crawler