import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.action_chains import ActionChains
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import pandas as pd
import time
import os
from datetime import datetime, timedelta, timezone
from supabase import create_client, Client
from pyvirtualdisplay import Display 

# ==================================================================
# [ì„¤ì •] í™˜ê²½ë³€ìˆ˜
# ==================================================================
SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_KEY")
COMPLEX_NO = "108064"

if not SUPABASE_URL or not SUPABASE_KEY:
    print("âŒ Supabase ì„¤ì •ì´ ì—†ìŠµë‹ˆë‹¤.")
    exit()

supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

KST = timezone(timedelta(hours=9))
NOW = datetime.now(KST)
TODAY_STR = NOW.strftime("%Y-%m-%d")
HOUR_STR = NOW.strftime("%H")

def run_crawler():
    print(f"ğŸš€ [GitHub Actions] {TODAY_STR} {HOUR_STR}ì‹œ í¬ë¡¤ë§ ì‹œì‘...")

    display = Display(visible=0, size=(1920, 1080))
    display.start()
    
    options = uc.ChromeOptions()
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--window-size=1920,1080")
    options.add_argument("--lang=ko_KR")
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")

    driver = uc.Chrome(options=options)
    
    driver.execute_cdp_cmd("Page.addScriptToEvaluateOnNewDocument", {
        "source": """
            Object.defineProperty(navigator, 'webdriver', { get: () => undefined });
        """
    })
    
    try:
        driver.get(f"https://new.land.naver.com/complexes/{COMPLEX_NO}")
        
        try: WebDriverWait(driver, 40).until(EC.presence_of_element_located((By.ID, "complex_article_trad_type_filter_0")))
        except: print("âš ï¸ ë¡œë”© ì§€ì—°")

        # --- í•„í„° ì„¤ì • ---
        print("âš™ï¸ í•„í„° ì ìš© ì¤‘...")
        try:
            driver.execute_script("if(document.querySelector('#complex_article_trad_type_filter_0:checked')) document.querySelector('#complex_article_trad_type_filter_0').click();")
            time.sleep(0.5)
            driver.execute_script("if(!document.querySelector('#complex_article_trad_type_filter_1:checked')) document.querySelector('#complex_article_trad_type_filter_1').click();")
            time.sleep(1)
            
            # [ë™ì¼ë§¤ë¬¼ ë¬¶ê¸°] ì²´í¬ (í•„ìˆ˜)
            group_input = driver.find_element(By.ID, "address_group2")
            if not group_input.is_selected():
                print("   ğŸ‘‰ [ë™ì¼ë§¤ë¬¼ ë¬¶ê¸°] í´ë¦­")
                driver.execute_script("arguments[0].click();", driver.find_element(By.CSS_SELECTOR, "label[for='address_group2']"))
                time.sleep(1)
            
            # ê°€ê²©ìˆœ ì •ë ¬
            driver.find_element(By.CSS_SELECTOR, "a.sorting_type[data-nclk='TAA.price']").click()
            
            print("   â³ ëª©ë¡ ê°±ì‹  ëŒ€ê¸° (5ì´ˆ)...")
            time.sleep(5)

        except Exception as e:
            print(f"âš ï¸ í•„í„° ì˜¤ë¥˜: {e}")
        
        # --- ìŠ¤í¬ë¡¤ ë¡œì§ ---
        print("â¬‡ï¸ ë°ì´í„° ë¡œë”© ì¤‘...")
        try: 
            list_area = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, "articleListArea")))
            actions = ActionChains(driver)
            actions.move_to_element(list_area).click().perform()
        except: 
            list_area = driver.find_element(By.TAG_NAME, "body")

        last_count = 0
        same_count_loop = 0
        
        while True:
            driver.execute_script("arguments[0].scrollTop = arguments[0].scrollHeight", list_area)
            try: 
                list_area.send_keys(Keys.END)
                time.sleep(0.3)
                list_area.send_keys(Keys.PAGE_DOWN)
            except: pass
            
            time.sleep(2.0)
            
            items = driver.find_elements(By.CSS_SELECTOR, "div.item:not(.item--child)")
            current_count = len(items)
            print(f"   ... ìŠ¤í¬ë¡¤ ì¤‘ (í˜„ì¬ {current_count}ê°œ ê·¸ë£¹)")

            if current_count == last_count and current_count > 0:
                same_count_loop += 1
                if same_count_loop >= 5:
                    print("   âœ… ìŠ¤í¬ë¡¤ ì™„ë£Œ")
                    break
            else:
                same_count_loop = 0
            last_count = current_count

        # --- ë°ì´í„° ì¶”ì¶œ (í•µì‹¬ ìˆ˜ì •) ---
        parent_items = driver.find_elements(By.CSS_SELECTOR, "div.item:not(.item--child)")
        print(f"ğŸ“ ì´ {len(parent_items)}ê°œ ê·¸ë£¹ ë°œê²¬. ìƒì„¸ ìˆ˜ì§‘ ì‹œì‘...")
        
        if len(parent_items) == 0:
            print("âŒ ë°ì´í„° 0ê±´.")
            return

        db_data = []
        
        for idx, parent in enumerate(parent_items):
            try:
                # ë¶€ëª¨ ì •ë³´ íŒŒì‹± (ë™, ìŠ¤í™, ê°€ê²© ë²”ìœ„ ë“±)
                p_html = parent.get_attribute('outerHTML')
                soup = BeautifulSoup(p_html, "html.parser")
                
                try: title = soup.select_one("div.item_title > span.text").get_text(strip=True)
                except: continue
                if title == "ì œëª©ì—†ìŒ": continue
                
                dong = title.replace("DMCíŒŒí¬ë·°ìì´", "").strip()
                
                try: spec = soup.select_one("div.info_area .spec").get_text(strip=True)
                except: spec = ""

                # ======================================================
                # [í•µì‹¬] ìì‹ ë§¤ë¬¼(ê°œë³„ ì¤‘ê°œì‚¬) ëª¨ë‘ ê¸ì–´ì˜¤ê¸° ë¡œì§
                # ======================================================
                
                # 1. "ì¤‘ê°œì‚¬ Nê³³" ë²„íŠ¼ì´ ìˆëŠ”ì§€ í™•ì¸
                multi_btn = parent.find_elements(By.CSS_SELECTOR, "span.label--multicp")
                
                targets = [] # ì •ë³´ë¥¼ ì¶”ì¶œí•  ëŒ€ìƒ ìš”ì†Œë“¤ ë¦¬ìŠ¤íŠ¸

                if multi_btn:
                    # ë¬¶ìŒ ë§¤ë¬¼ì´ë©´ -> í¼ì¹˜ê¸° ë²„íŠ¼ í´ë¦­!
                    driver.execute_script("arguments[0].click();", multi_btn[0])
                    time.sleep(0.3) # í¼ì¹¨ ëŒ€ê¸°
                    
                    # í¼ì³ì§„ ìì‹ ì»¨í…Œì´ë„ˆ ì°¾ê¸°
                    # ì£¼ì˜: parent ì•ˆì— item--childê°€ ìƒì„±ë¨
                    try:
                        child_container = parent.find_element(By.CSS_SELECTOR, "div.item.item--child")
                        # ê·¸ ì•ˆì˜ ê°œë³„ ë§¤ë¬¼(item_inner)ë“¤ì„ ëª¨ë‘ ì°¾ìŒ
                        inners = child_container.find_elements(By.CSS_SELECTOR, "div.item_inner")
                        
                        # ë¡œë”©ë°” ë“± ê°€ì§œ ìš”ì†Œ ì œì™¸í•˜ê³  ì§„ì§œ ì •ë³´(cp_area) ìˆëŠ” ê²ƒë§Œ ë‹´ê¸°
                        for inner in inners:
                            if inner.find_elements(By.CSS_SELECTOR, "div.cp_area"):
                                targets.append(inner)
                    except:
                        # í¼ì¹˜ê¸° ì‹¤íŒ¨ì‹œ ë¶€ëª¨ë¼ë„ ë‹´ìŒ
                        targets.append(parent.find_element(By.CSS_SELECTOR, "div.item_inner"))
                else:
                    # ë‹¨ë… ë§¤ë¬¼ì´ë©´ -> ë¶€ëª¨ ìì‹ ì„ íƒ€ê²Ÿìœ¼ë¡œ
                    targets.append(parent.find_element(By.CSS_SELECTOR, "div.item_inner"))

                # 2. í™•ë³´ëœ íƒ€ê²Ÿë“¤(ê°œë³„ ì¤‘ê°œì‚¬ ë§¤ë¬¼) ìˆœíšŒí•˜ë©° ì €ì¥
                for target in targets:
                    t_html = target.get_attribute('outerHTML')
                    t_soup = BeautifulSoup(t_html, "html.parser")
                    
                    # ì¤‘ê°œì‚¬ ì´ë¦„
                    try: agent = t_soup.select("a.agent_name")[-1].get_text(strip=True)
                    except: agent = "ì•Œìˆ˜ì—†ìŒ"
                    
                    # ê°€ê²© (ê°œë³„ ê°€ê²©)
                    try: price = t_soup.select_one("span.price").get_text(strip=True)
                    except: 
                        # ê°œë³„ ê°€ê²© ì—†ìœ¼ë©´ ë¶€ëª¨ì˜ ê°€ê²© ë²”ìœ„ë¼ë„ ê°€ì ¸ì˜´
                        try: price = soup.select_one("span.price").get_text(strip=True)
                        except: price = "ê°€ê²©ì—†ìŒ"
                    
                    # ë§¤ë¬¼ë²ˆí˜¸ (í´ë¦­ ì•ˆí•˜ê³  ë¦¬ìŠ¤íŠ¸ì— ë…¸ì¶œëœ ì •ë³´ê°€ ìˆë‹¤ë©´ ì¢‹ê² ì§€ë§Œ, ë³´í†µ í´ë¦­í•´ì•¼ ë‚˜ì˜´)
                    # ì—¬ê¸°ì„œëŠ” ì†ë„ë¥¼ ìœ„í•´ "-"ë¡œ ë‘ê±°ë‚˜, í•„ìš”ì‹œ í´ë¦­ ë¡œì§ ì¶”ê°€
                    article_no = "-" 
                    
                    # DB ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
                    db_data.append({
                        "agent": agent, 
                        "dong": dong, 
                        "spec": spec, 
                        "price": price,
                        "article_no": article_no, 
                        "crawl_date": TODAY_STR, 
                        "crawl_time": f"{HOUR_STR}ì‹œ"
                    })

            except Exception as e:
                continue # íŠ¹ì • ë§¤ë¬¼ ì—ëŸ¬ë‚˜ë„ ë‹¤ìŒìœ¼ë¡œ ë„˜ì–´ê°
        
        driver.quit()

        # ======================================================
        # DB ì €ì¥
        # ======================================================
        if db_data:
            # 1. ìƒì„¸ ë¡œê·¸ (real_estate_logs)
            try:
                supabase.table('real_estate_logs').insert(db_data).execute()
                print(f"âœ… [Log] ì´ {len(db_data)}ê±´ ì €ì¥ ì™„ë£Œ")
            except Exception as e:
                print(f"âŒ [Log] ì €ì¥ ì‹¤íŒ¨: {e}")

            # 2. í†µê³„ ì €ì¥ (agent_stats)
            df = pd.DataFrame(db_data)
            stats_df = df['agent'].value_counts().reset_index()
            stats_df.columns = ['agent', 'count']
            
            stats_data = []
            for _, row in stats_df.iterrows():
                stats_data.append({
                    "agent": row['agent'],
                    "count": int(row['count']),
                    "crawl_date": TODAY_STR,
                    "crawl_time": f"{HOUR_STR}ì‹œ"
                })
            
            try:
                supabase.table('agent_stats').insert(stats_data).execute()
                print(f"âœ… [Stats] í†µê³„ ì €ì¥ ì™„ë£Œ")
            except: pass
        else:
            print("âŒ ìˆ˜ì§‘ëœ ë°ì´í„° 0ê±´")

    except Exception as e:
        print(f"âŒ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        driver.quit()
    finally:
        display.stop()

if __name__ == "__main__":
    run_crawler()