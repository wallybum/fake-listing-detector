import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.action_chains import ActionChains
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import pandas as pd
import time
import os
from datetime import datetime, timedelta, timezone
from supabase import create_client, Client

# ==================================================================
# [ì„¤ì •] í™˜ê²½ë³€ìˆ˜
# ==================================================================
SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_KEY")
COMPLEX_NO = "108064"

if not SUPABASE_URL or not SUPABASE_KEY:
    print("âŒ Supabase ì„¤ì •ì´ ì—†ìŠµë‹ˆë‹¤.")
    exit()

supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

# [ìˆ˜ì • 1] í•œêµ­ ì‹œê°„(KST) ì ìš©ì„ í™•ì‹¤í•˜ê²Œ!
KST = timezone(timedelta(hours=9))
NOW = datetime.now(KST) 
TODAY_STR = NOW.strftime("%Y-%m-%d")
HOUR_STR = NOW.strftime("%H")

def run_crawler():
    print(f"ğŸš€ [GitHub Actions] {TODAY_STR} {HOUR_STR}ì‹œ í¬ë¡¤ë§ ì‹œì‘...")
    
    # [ìˆ˜ì • 2] íƒì§€ íšŒí”¼ ì˜µì…˜ ê°•í™”
    options = uc.ChromeOptions()
    options.add_argument("--headless=new") # ìµœì‹  í—¤ë“œë¦¬ìŠ¤
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--window-size=1920,1080")
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_argument("--disable-extensions")
    options.add_argument("user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")

    driver = uc.Chrome(options=options)
    
    try:
        driver.get(f"https://new.land.naver.com/complexes/{COMPLEX_NO}")
        
        # ë¡œë”© ëŒ€ê¸° (30ì´ˆ)
        try: WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.ID, "complex_article_trad_type_filter_0")))
        except: print("âš ï¸ ë¡œë”© ì‹œê°„ ì´ˆê³¼ (ê³„ì† ì§„í–‰)")

        # í•„í„°
        try:
            driver.execute_script("if(document.querySelector('#complex_article_trad_type_filter_0:checked')) document.querySelector('#complex_article_trad_type_filter_0').click();")
            time.sleep(0.5)
            driver.execute_script("if(!document.querySelector('#complex_article_trad_type_filter_1:checked')) document.querySelector('#complex_article_trad_type_filter_1').click();")
            time.sleep(1)
            driver.execute_script("""var cb = document.getElementById("address_group2"); if (cb && !cb.checked) document.querySelector("label[for='address_group2']").click();""")
            time.sleep(1)
            driver.find_element(By.CSS_SELECTOR, "a.sorting_type[data-nclk='TAA.price']").click()
        except: pass
        
        time.sleep(3)

        # ìŠ¤í¬ë¡¤
        print("â¬‡ï¸ ë°ì´í„° ë¡œë”© ì¤‘...")
        try: 
            list_area = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, "articleListArea")))
            actions = ActionChains(driver)
            actions.move_to_element(list_area).click().perform()
        except: 
            list_area = driver.find_element(By.TAG_NAME, "body")

        last_count = 0
        same_count_loop = 0
        
        # ìµœëŒ€ 60ì´ˆ ë™ì•ˆ ì‹œë„
        for _ in range(30):
            driver.execute_script("arguments[0].scrollTop = arguments[0].scrollHeight", list_area)
            try: list_area.send_keys(Keys.END)
            except: pass
            time.sleep(2.0)
            
            items = driver.find_elements(By.CSS_SELECTOR, "div.item:not(.item--child)")
            current_count = len(items)
            print(f"   ... ë¡œë”© ì¤‘ ({current_count}ê°œ)")

            if current_count == last_count and current_count > 0:
                same_count_loop += 1
                if same_count_loop >= 3: break
            else:
                same_count_loop = 0
            last_count = current_count

        # ë°ì´í„° ì¶”ì¶œ
        parent_items = driver.find_elements(By.CSS_SELECTOR, "div.item:not(.item--child)")
        print(f"ğŸ“ ì´ {len(parent_items)}ê°œ ê·¸ë£¹ ë°œê²¬.")

        # [ìˆ˜ì • 3] ë°ì´í„°ê°€ 0ê°œë©´ ìŠ¤í¬ë¦°ìƒ· ì°ê¸° (ë””ë²„ê¹… í•µì‹¬)
        if len(parent_items) == 0:
            print("âŒ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. í˜„ì¬ í™”ë©´ì„ ìº¡ì²˜í•©ë‹ˆë‹¤.")
            driver.save_screenshot("debug_screenshot.png")
            
            # HTML ì†ŒìŠ¤ë„ ì¼ë¶€ ì €ì¥ (ì°¨ë‹¨ ë¬¸êµ¬ í™•ì¸ìš©)
            with open("debug_source.html", "w", encoding="utf-8") as f:
                f.write(driver.page_source)
            
            driver.quit()
            return

        db_data = []
        # ... (ê¸°ì¡´ íŒŒì‹± ë¡œì§ ë™ì¼) ...
        
        def get_article_no():
            for _ in range(3):
                try:
                    time.sleep(0.2)
                    soup = BeautifulSoup(driver.page_source, "html.parser")
                    target_th = soup.find("th", string=lambda t: t and "ë§¤ë¬¼ë²ˆí˜¸" in t)
                    if target_th: return target_th.find_next_sibling("td").get_text(strip=True)
                except: pass
            return "-"

        for parent in parent_items:
            try:
                p_soup = BeautifulSoup(parent.get_attribute('outerHTML'), "html.parser")
                try: p_title = p_soup.select_one("div.item_title > span.text").get_text(strip=True)
                except: continue
                if p_title == "ì œëª©ì—†ìŒ": continue
                dong_name = p_title.replace("DMCíŒŒí¬ë·°ìì´", "").strip()
                
                try: raw_spec = p_soup.select_one("div.info_area .spec").get_text(strip=True)
                except: raw_spec = ""

                multi_cp_btn = parent.find_elements(By.CSS_SELECTOR, "span.label--multicp")
                targets = []
                if multi_cp_btn:
                    driver.execute_script("arguments[0].click();", multi_cp_btn[0])
                    time.sleep(0.3)
                    driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", parent)
                    child_container = parent.find_element(By.CSS_SELECTOR, "div.item.item--child")
                    children = child_container.find_elements(By.CSS_SELECTOR, "div.item_inner")
                    for child in children:
                        if child.find_elements(By.CSS_SELECTOR, "div.cp_area"): targets.append(child)
                else:
                    targets.append(parent.find_element(By.CSS_SELECTOR, "div.item_inner"))

                for target in targets:
                    t_soup = BeautifulSoup(target.get_attribute('outerHTML'), "html.parser")
                    try: agent = t_soup.select("a.agent_name")[-1].get_text(strip=True)
                    except: agent = "ì•Œìˆ˜ì—†ìŒ"
                    try: price = t_soup.select_one("span.price").get_text(strip=True)
                    except: price = ""
                    
                    article_no = "-" # ì†ë„ë¥¼ ìœ„í•´ ì¼ë‹¨ ìƒëµí•˜ê±°ë‚˜, í•„ìš”í•˜ë©´ í´ë¦­ ë¡œì§ ì¶”ê°€
                    
                    db_data.append({
                        "agent": agent, "dong": dong_name, "spec": raw_spec, "price": price,
                        "article_no": article_no, "crawl_date": TODAY_STR, "crawl_time": f"{HOUR_STR}ì‹œ"
                    })
            except: continue

        driver.quit()

        if db_data:
            try:
                supabase.table('real_estate_logs').insert(db_data).execute()
                print(f"âœ… [Log Table] {len(db_data)}ê±´ ì €ì¥ ì™„ë£Œ")
            except Exception as e:
                print(f"âŒ [Log Table] ì‹¤íŒ¨: {e}")

            # í†µê³„ ì €ì¥
            df = pd.DataFrame(db_data)
            stats_df = df['agent'].value_counts().reset_index()
            stats_df.columns = ['agent', 'count']
            stats_data = []
            for _, row in stats_df.iterrows():
                stats_data.append({
                    "agent": row['agent'], "count": int(row['count']),
                    "crawl_date": TODAY_STR, "crawl_time": f"{HOUR_STR}ì‹œ"
                })
            try:
                supabase.table('agent_stats').insert(stats_data).execute()
                print(f"âœ… [Stats Table] í†µê³„ ì €ì¥ ì™„ë£Œ")
            except: pass
    
    except Exception as e:
        print(f"âŒ ì „ì²´ ì˜¤ë¥˜: {e}")
        driver.quit()

if __name__ == "__main__":
    run_crawler()