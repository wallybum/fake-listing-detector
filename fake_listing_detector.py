import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.action_chains import ActionChains
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import pandas as pd
import time
import os
from datetime import datetime, timedelta, timezone
from supabase import create_client, Client

# [í•„ìˆ˜] ê°€ìƒ ëª¨ë‹ˆí„° (ì„œë²„ì—ì„œ 0ê±´ ëœ¨ëŠ” ë¬¸ì œ í•´ê²°ìš©)
from pyvirtualdisplay import Display 

# ==================================================================
# [ì„¤ì •] í™˜ê²½ë³€ìˆ˜
# ==================================================================
SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_KEY")
COMPLEX_NO = "108064" # DMCíŒŒí¬ë·°ìì´

if not SUPABASE_URL or not SUPABASE_KEY:
    print("âŒ Supabase ì„¤ì •ì´ ì—†ìŠµë‹ˆë‹¤.")
    exit()

supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

# í•œêµ­ ì‹œê°„ ì„¤ì •
KST = timezone(timedelta(hours=9))
NOW = datetime.now(KST)
TODAY_STR = NOW.strftime("%Y-%m-%d")
HOUR_STR = NOW.strftime("%H")

def run_crawler():
    print(f"ğŸš€ [GitHub Actions] {TODAY_STR} {HOUR_STR}ì‹œ í¬ë¡¤ë§ ì‹œì‘...")

    # 1. ê°€ìƒ ëª¨ë‹ˆí„° ì¼œê¸° (ì„œë²„ì—ì„œë„ í™”ë©´ì´ ìˆëŠ” ì²™ ì†ì„ -> ë´‡ ì°¨ë‹¨ íšŒí”¼ í•µì‹¬)
    display = Display(visible=0, size=(1920, 1080))
    display.start()
    
    options = uc.ChromeOptions()
    # [ì¤‘ìš”] --headless ì˜µì…˜ ì œê±°! (ê°€ìƒ í™”ë©´ì„ ì“°ë¯€ë¡œ í™”ë©´ ìˆëŠ” ëª¨ë“œë¡œ ì‹¤í–‰)
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--window-size=1920,1080")
    # ë´‡ íƒì§€ ë°©ì§€
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")

    driver = uc.Chrome(options=options)
    
    try:
        # 2. ì ‘ì†
        driver.get(f"https://new.land.naver.com/complexes/{COMPLEX_NO}")
        
        try: WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.ID, "complex_article_trad_type_filter_0")))
        except: pass

        # ------------------------------------------------------------------
        # 3. í•„í„° ì„¤ì • (ë¡œì»¬ì—ì„œ ì„±ê³µí•œ ë¡œì§ ê·¸ëŒ€ë¡œ ì ìš©)
        # ------------------------------------------------------------------
        print("âš™ï¸ í•„í„° ì ìš© ì¤‘...")
        try:
            # ì „ì²´ í•´ì œ
            driver.execute_script("if(document.querySelector('#complex_article_trad_type_filter_0:checked')) document.querySelector('#complex_article_trad_type_filter_0').click();")
            time.sleep(0.5)
            # ë§¤ë§¤ ì„ íƒ
            driver.execute_script("if(!document.querySelector('#complex_article_trad_type_filter_1:checked')) document.querySelector('#complex_article_trad_type_filter_1').click();")
            time.sleep(1)
            # [í•µì‹¬] ë™ì¼ë§¤ë¬¼ ë¬¶ê¸° (ë¡œì»¬ ì„±ê³µ ì½”ë“œ)
            driver.execute_script("""var cb = document.getElementById("address_group2"); if (cb && !cb.checked) document.querySelector("label[for='address_group2']").click();""")
            time.sleep(1)
            # ê°€ê²©ìˆœ ì •ë ¬
            driver.find_element(By.CSS_SELECTOR, "a.sorting_type[data-nclk='TAA.price']").click()
        except: pass
        
        time.sleep(3) # í•„í„° ì ìš© ëŒ€ê¸°

        # ------------------------------------------------------------------
        # 4. ìŠ¤í¬ë¡¤ ë¡œì§ (ë¡œì»¬ì—ì„œ ì„±ê³µí•œ 'ê°•ë ¥ ëª¨ë“œ' ì ìš©)
        # ------------------------------------------------------------------
        print("â¬‡ï¸ ë°ì´í„° ë¡œë”© ì¤‘ (ì „ì²´ ë§¤ë¬¼ í™•ë³´)...")
        try: 
            list_area = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, "articleListArea")))
            actions = ActionChains(driver)
            actions.move_to_element(list_area).click().perform()
        except: 
            list_area = driver.find_element(By.TAG_NAME, "body")

        last_count = 0
        same_count_loop = 0
        
        while True:
            # JS ìŠ¤í¬ë¡¤
            driver.execute_script("arguments[0].scrollTop = arguments[0].scrollHeight", list_area)
            
            # í‚¤ë³´ë“œ ì…ë ¥ (END + PAGE_DOWN ì½¤ë³´)
            try: 
                list_area.send_keys(Keys.END)
                time.sleep(0.5)
                list_area.send_keys(Keys.PAGE_DOWN)
            except: pass
            
            time.sleep(2.0) # ì¶©ë¶„í•œ ëŒ€ê¸°
            
            # ë¶€ëª¨ ê·¸ë£¹ ê°œìˆ˜ í™•ì¸
            items = driver.find_elements(By.CSS_SELECTOR, "div.item:not(.item--child)")
            current_count = len(items)
            
            print(f"   ... ìŠ¤í¬ë¡¤ ì¤‘ (í˜„ì¬ {current_count}ê°œ ê·¸ë£¹ ë¡œë”©ë¨)")

            if current_count == last_count:
                same_count_loop += 1
                # 5ë²ˆ ì—°ì† ë³€í™” ì—†ìœ¼ë©´ ì¢…ë£Œ
                if same_count_loop >= 5:
                    print("   âœ… ì „ì²´ ëª©ë¡ ë¡œë”© ì™„ë£Œ.")
                    break
            else:
                same_count_loop = 0
                
            last_count = current_count

        # ------------------------------------------------------------------
        # 5. ë°ì´í„° ì¶”ì¶œ (ë¡œì»¬ ì„±ê³µ ë¡œì§: í¼ì¹˜ê¸° í¬í•¨)
        # ------------------------------------------------------------------
        parent_items = driver.find_elements(By.CSS_SELECTOR, "div.item:not(.item--child)")
        print(f"ğŸ“ ì´ {len(parent_items)}ê°œ ê·¸ë£¹ ë°œê²¬. DB ì „ì†¡ ì¤€ë¹„...")
        
        db_data = []

        def get_article_no():
            for _ in range(3):
                try:
                    time.sleep(0.3)
                    soup = BeautifulSoup(driver.page_source, "html.parser")
                    target_th = soup.find("th", string=lambda t: t and "ë§¤ë¬¼ë²ˆí˜¸" in t)
                    if target_th: return target_th.find_next_sibling("td").get_text(strip=True)
                except: pass
            return "-"

        for parent in parent_items:
            try:
                p_soup = BeautifulSoup(parent.get_attribute('outerHTML'), "html.parser")
                try: p_title = p_soup.select_one("div.item_title > span.text").get_text(strip=True)
                except: continue
                if p_title == "ì œëª©ì—†ìŒ": continue
                dong_name = p_title.replace("DMCíŒŒí¬ë·°ìì´", "").strip()
                
                try: raw_spec = p_soup.select_one("div.info_area .spec").get_text(strip=True)
                except: raw_spec = ""

                # í¼ì¹˜ê¸° (ì¤‘ìš”!)
                multi_cp_btn = parent.find_elements(By.CSS_SELECTOR, "span.label--multicp")
                targets = []
                
                if multi_cp_btn:
                    driver.execute_script("arguments[0].click();", multi_cp_btn[0])
                    time.sleep(0.3)
                    driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", parent)
                    child_container = parent.find_element(By.CSS_SELECTOR, "div.item.item--child")
                    children = child_container.find_elements(By.CSS_SELECTOR, "div.item_inner")
                    for child in children:
                        if child.find_elements(By.CSS_SELECTOR, "div.cp_area"): targets.append(child)
                else:
                    targets.append(parent.find_element(By.CSS_SELECTOR, "div.item_inner"))

                for target in targets:
                    t_soup = BeautifulSoup(target.get_attribute('outerHTML'), "html.parser")
                    try: agent = t_soup.select("a.agent_name")[-1].get_text(strip=True)
                    except: agent = "ì•Œìˆ˜ì—†ìŒ"
                    try: price = t_soup.select_one("span.price").get_text(strip=True)
                    except: price = ""

                    # ìƒì„¸ í˜ì´ì§€ í´ë¦­ (ë§¤ë¬¼ë²ˆí˜¸ í™•ë³´ìš©)
                    cp_btns = target.find_elements(By.CSS_SELECTOR, "a.label.label--cp")
                    if cp_btns: driver.execute_script("arguments[0].click();", cp_btns[0])
                    else: driver.execute_script("arguments[0].click();", target.find_element(By.CSS_SELECTOR, "a.item_link"))
                    
                    article_no = get_article_no()
                    
                    db_data.append({
                        "agent": agent,
                        "dong": dong_name,
                        "spec": raw_spec,
                        "price": price,
                        "article_no": article_no,
                        "crawl_date": TODAY_STR,
                        "crawl_time": f"{HOUR_STR}ì‹œ"
                    })
            except: continue

        # ------------------------------------------------------------------
        # 6. DB ì €ì¥ (ì—‘ì…€ ì—†ì´ Supabaseë¡œ ì§í–‰)
        # ------------------------------------------------------------------
        if not db_data:
            print("âŒ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            driver.save_screenshot("debug_no_data.png")
            return

        # [ì €ì¥ 1] ìƒì„¸ ë¡œê·¸
        try:
            supabase.table('real_estate_logs').insert(db_data).execute()
            print(f"âœ… [Log Table] ìƒì„¸ ë§¤ë¬¼ {len(db_data)}ê±´ ì €ì¥ ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ [Log Table] ì €ì¥ ì‹¤íŒ¨: {e}")

        # [ì €ì¥ 2] í†µê³„ ë°ì´í„°
        df = pd.DataFrame(db_data)
        stats_df = df['agent'].value_counts().reset_index()
        stats_df.columns = ['agent', 'count']
        
        stats_data = []
        for _, row in stats_df.iterrows():
            stats_data.append({
                "agent": row['agent'],
                "count": int(row['count']),
                "crawl_date": TODAY_STR,
                "crawl_time": f"{HOUR_STR}ì‹œ"
            })
        
        try:
            supabase.table('agent_stats').insert(stats_data).execute()
            print(f"âœ… [Stats Table] ì¤‘ê°œì‚¬ {len(stats_data)}ê³³ í†µê³„ ì €ì¥ ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ [Stats Table] ì €ì¥ ì‹¤íŒ¨: {e}")

    except Exception as e:
        print(f"âŒ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        driver.save_screenshot("debug_error.png")
        
    finally:
        driver.quit()
        display.stop() # ê°€ìƒ ëª¨ë‹ˆí„° ì¢…ë£Œ

if __name__ == "__main__":
    run_crawler()