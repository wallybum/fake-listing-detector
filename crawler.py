import os
import json
import time
import sys
from datetime import datetime, timedelta, timezone
from pathlib import Path

import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.common.action_chains import ActionChains
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

from dotenv import load_dotenv
from supabase import create_client, Client

# ==================================================================
# [ì„¤ì •] í™˜ê²½ë³€ìˆ˜ ë° ìƒìˆ˜ ì •ì˜
# ==================================================================
COMPLEX_NO = "108064"
KST = timezone(timedelta(hours=9))

# 1. í˜„ì¬ íŒŒì¼ ìœ„ì¹˜ ê¸°ì¤€ .env.local ë¡œë“œ
current_dir = os.path.dirname(os.path.abspath(__file__))
env_path = os.path.join(current_dir, 'land-dashboard/.env.local') 

load_result = load_dotenv(dotenv_path=env_path)
print(f"ğŸ“‚ ê²½ë¡œ: {env_path}")
print(f"ğŸ”„ ë¡œë“œ ê²°ê³¼: {load_result}")

SUPABASE_URL = os.environ.get("NEXT_PUBLIC_SUPABASE_URL")
SUPABASE_KEY = os.environ.get("NEXT_PUBLIC_SUPABASE_KEY")

if SUPABASE_URL:
    print(f"âœ… URL ë¡œë“œ ì„±ê³µ: {SUPABASE_URL[:10]}...")
else:
    print("âŒ URL ë¡œë“œ ì‹¤íŒ¨ (DB ì €ì¥ ë¶ˆê°€)")


# ==================================================================
# [í•¨ìˆ˜] ë°ì´í„° ì •ì œ ë° DB ì €ì¥
# ==================================================================
def refine_data(raw_data_list, trade_type, fixed_date, fixed_time):
    """
    ë„¤ì´ë²„ ì›ë³¸ ë°ì´í„° ë¦¬ìŠ¤íŠ¸ë¥¼ DB ìŠ¤í‚¤ë§ˆì— ë§ê²Œ ë³€í™˜
    """
    refined_list = []
    
    for item in raw_data_list:
        # ê°€ê²© ì •ë³´
        price_str = item.get('dealOrWarrantPrc', '')
        
        area_name = item.get('areaName', '')   # 110E-2
        area_ex = item.get('area2', '')        # 84 (ì „ìš©ë©´ì )
        floor = item.get('floorInfo', '')      # ì €/22ì¸µ
        direction = item.get('direction', '')  # ë‚¨ì„œí–¥
        formatted_spec = f"{area_name}/{area_ex}mÂ², {floor}, {direction}"

        refined_item = {
             "crawl_date": fixed_date,
             "crawl_time": fixed_time,
             "article_no": item.get('articleNo', ''),  # ë§¤ë¬¼ ë²ˆí˜¸ (PK)
             "trade_type": trade_type,                 # ë§¤ë§¤/ì „ì„¸
             "price": price_str,                       # ê°€ê²©
             "dong": item.get('buildingName'),         # ë™
             "spec": formatted_spec,
             "agent": item.get('realtorName'),         # ì¤‘ê°œì—…ì†Œ
             "provider": item.get('cpName'),           # ì œê³µ ì—…ì²´
             "confirm_date": item.get('articleConfirmYmd',''), # í™•ì¸ë‚ ì§œ
             "is_owner": item.get('verificationTypeCode') == 'OWNER' # ì§‘ì£¼ì¸ ì¸ì¦ì—¬ë¶€
        }
        refined_list.append(refined_item)
    
    return refined_list

def save_to_supabase(data_list):
    """
    Supabase DBì— ë§¤ë¬¼ ë°ì´í„° ì €ì¥ (Upsert)
    """
    if not data_list or not SUPABASE_URL:
        print("âš ï¸ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ê±°ë‚˜ DB ì„¤ì •ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return

    try:
        supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
        table_name = "real_estate_logs" 

        response = supabase.table(table_name).upsert(data_list).execute()
        
        print(f"âœ… DB ì €ì¥ ì™„ë£Œ! (ì´ {len(data_list)}ê±´ ì²˜ë¦¬)")
        
    except Exception as e:
        print(f"âŒ DB ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

# [ì¶”ê°€ë¨] ì´ë ¥ ê¸°ë¡ í•¨ìˆ˜
def save_crawl_history(date, time_str, status, count=0, error_msg=""):
    """
    crawl_history í…Œì´ë¸”ì— ì„±ê³µ/ì‹¤íŒ¨ ì—¬ë¶€ë¥¼ ê¸°ë¡í•©ë‹ˆë‹¤.
    """
    if not SUPABASE_URL: return

    try:
        supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
        
        history_data = {
            "crawl_date": date,
            "crawl_time": time_str,
            "status": status,          # 'SUCCESS' ë˜ëŠ” 'FAIL'
            "collected_count": count,  # ìˆ˜ì§‘ëœ ê°œìˆ˜
            "error_message": str(error_msg)[:1000] # ì—ëŸ¬ ë©”ì‹œì§€ ê¸¸ì´ ì œí•œ
        }
        
        supabase.table("crawl_history").insert(history_data).execute()
        print(f"ğŸ“ [History] ì´ë ¥ ê¸°ë¡ ì™„ë£Œ: {status} ({count}ê±´)")
        
    except Exception as e:
        print(f"âŒ ì´ë ¥ ê¸°ë¡ ì‹¤íŒ¨: {e}")


# ==================================================================
# [í´ë˜ìŠ¤] í¬ë¡¤ëŸ¬ ì •ì˜
# ==================================================================
class NaverLandCrawler:
    
    def __init__(self):
        """ìƒì„±ì: ë“œë¼ì´ë²„ ì´ˆê¸°í™”"""
        self.driver = self._init_driver()

    def _init_driver(self):
        """ë“œë¼ì´ë²„ ì˜µì…˜ ì„¤ì •"""
        options = uc.ChromeOptions()
        options.add_argument("--headless=new") 
        options.add_argument("--window-size=1920,1080")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--disable-gpu")
        options.add_argument("--lang=ko_KR")
        options.add_argument("--disable-blink-features=AutomationControlled")
        
        prefs = {"profile.managed_default_content_settings.images": 2}
        options.add_experimental_option("prefs", prefs)
        options.set_capability("goog:loggingPrefs", {"performance": "ALL"})

        # ì •ì‹ ë²„ì „ ì‚¬ìš© ê¶Œì¥ (ë²„ì „ ëª…ì‹œ)
        driver = uc.Chrome(options=options, version_main=142) 
        
        ua = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        driver.execute_cdp_cmd("Network.setUserAgentOverride", {
            "userAgent": ua,
            "acceptLanguage": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
            "platform": "MacIntel"
        })
        driver.execute_cdp_cmd("Network.setExtraHTTPHeaders", {
            "headers": {
                "Referer": "https://new.land.naver.com/",
                "Origin": "https://new.land.naver.com"
            }
        })
        
        return driver

    def close(self):
        # ë“œë¼ì´ë²„ê°€ ì¡´ì¬í•˜ê³  ì‚´ì•„ìˆì„ ë•Œë§Œ ì¢…ë£Œ ì‹œë„
        if hasattr(self, 'driver') and self.driver:
            try:
                print("\nğŸ‘‹ í¬ë¡¤ëŸ¬ ì¢…ë£Œ (ë¸Œë¼ìš°ì € ë‹«ê¸°)")
                self.driver.quit()
            except Exception:
                pass # ì´ë¯¸ ë‹«í˜€ìˆìœ¼ë©´ íŒ¨ìŠ¤

    def _wait_for_loading(self):
        try:
            WebDriverWait(self.driver, 20).until(EC.presence_of_element_located((By.ID, "articleListArea")))
        except Exception as e:
            print(f"   âš ï¸ ë¡œë”© ëŒ€ê¸° ì‹¤íŒ¨: {e}")

    def _reset_and_apply_filters(self, target_type):
        print(f"   âš™ï¸ í•„í„° ì ìš© ì¤‘: {target_type}")
        
        # 1. ì „ì²´ ê±°ë˜ë°©ì‹ í•´ì œ
        self.driver.execute_script("if(document.querySelector('#complex_article_trad_type_filter_0:checked')) { document.querySelector('#complex_article_trad_type_filter_0').click(); }")
        time.sleep(0.5)

        # 2. íƒ€ê²Ÿ íƒ€ì… ì„¤ì •
        if (target_type == "ë§¤ë§¤"):
            self.driver.execute_script("if(!document.querySelector('#complex_article_trad_type_filter_1:checked')) { document.querySelector('#complex_article_trad_type_filter_1').click(); }")
            self.driver.execute_script("if(document.querySelector('#complex_article_trad_type_filter_2:checked')) { document.querySelector('#complex_article_trad_type_filter_2').click(); }")
        
        elif (target_type == "ì „ì„¸"):
            self.driver.execute_script("if(document.querySelector('#complex_article_trad_type_filter_1:checked')) { document.querySelector('#complex_article_trad_type_filter_1').click(); }")
            self.driver.execute_script("if(!document.querySelector('#complex_article_trad_type_filter_2:checked')) { document.querySelector('#complex_article_trad_type_filter_2').click(); }")

        time.sleep(1)

        # 3. ë¬¶ê¸° í•´ì œ
        try:
            group_chk = self.driver.find_element(By.ID, "address_group2")
            if (group_chk.is_selected()):
                self.driver.execute_script("arguments[0].click();", self.driver.find_element(By.CSS_SELECTOR, "label[for='address_group2']"))
        except:
            pass

        # 4. ê°€ê²©ìˆœ ì •ë ¬
        try:
            self.driver.find_element(By.CSS_SELECTOR, "a.sorting_type[data-nclk='TAA.price']").click()
        except:
            pass
        
        time.sleep(3)

    # ìˆ˜ì • 
    # def _reset_and_apply_filters(self, target_type):
        print(f"   âš™ï¸ í•„í„° ì ìš© ì¤‘: {target_type}")
    
        # [ìˆ˜ì •] ìš”ì†Œê°€ ë¡œë“œë  ë•Œê¹Œì§€ ëª…ì‹œì ìœ¼ë¡œ ê¸°ë‹¤ë¦¼
        wait = WebDriverWait(self.driver, 15)
        try:
            # í•„í„° ì˜ì—­ ìì²´ê°€ ë‚˜íƒ€ë‚  ë•Œê¹Œì§€ ëŒ€ê¸°
            wait.until(EC.presence_of_element_located((By.ID, "complex_article_trad_type_filter_0")))
            
            # 1. ì „ì²´ ê±°ë˜ë°©ì‹ í•´ì œ (ìš”ì†Œê°€ ìˆì„ ë•Œë§Œ ì‹¤í–‰í•˜ë„ë¡ JS ë³´ê°•)
            self.driver.execute_script("""
                var allBtn = document.querySelector('#complex_article_trad_type_filter_0');
                if(allBtn && allBtn.checked) { allBtn.click(); }
            """)
            time.sleep(0.8)

            # 2. íƒ€ê²Ÿ íƒ€ì… ì„¤ì • (ë§¤ë§¤/ì „ì„¸)
            if target_type == "ë§¤ë§¤":
                self.driver.execute_script("""
                    var maeBtn = document.querySelector('#complex_article_trad_type_filter_1');
                    var jeonBtn = document.querySelector('#complex_article_trad_type_filter_2');
                    if(maeBtn && !maeBtn.checked) { maeBtn.click(); }
                    if(jeonBtn && jeonBtn.checked) { jeonBtn.click(); }
                """)
            elif target_type == "ì „ì„¸":
                self.driver.execute_script("""
                    var maeBtn = document.querySelector('#complex_article_trad_type_filter_1');
                    var jeonBtn = document.querySelector('#complex_article_trad_type_filter_2');
                    if(maeBtn && maeBtn.checked) { maeBtn.click(); }
                    if(jeonBtn && !jeonBtn.checked) { jeonBtn.click(); }
                """)
            
            time.sleep(1.5) # í•„í„° ì ìš© í›„ ë°ì´í„° ê°±ì‹  ëŒ€ê¸°

            # 3. ë¬¶ê¸° í•´ì œ (ë¼ë²¨ í´ë¦­ ë°©ì‹ì´ ë” ì•ˆì •ì ì„)
            self.driver.execute_script("""
                var groupChk = document.querySelector('#address_group2');
                var groupLabel = document.querySelector("label[for='address_group2']");
                if(groupChk && groupChk.checked && groupLabel) { groupLabel.click(); }
            """)

        except Exception as e:
            print(f"   âš ï¸ í•„í„° ì ìš© ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ë¬´ì‹œí•˜ê³  ì§„í–‰): {e}")

    def _scroll_and_collect_packets(self, target_type):
    #     try:
    #         list_area = self.driver.find_element(By.ID, "articleListArea")
    #     except:
    #         list_area = self.driver.find_element(By.TAG_NAME, "body")
            
    #     try:
    #         ActionChains(self.driver).move_to_element(list_area).click().perform()
    #     except:
    #         pass

    #     collected_data_map = {}
    #     last_count = 0
    #     same_loop = 0
        
    #     for i in range(50): # ìµœëŒ€ 50íšŒ ìŠ¤í¬ë¡¤
    #         items = self.driver.find_elements(By.CSS_SELECTOR, "div.item:not(.item--child)")
    #         curr_count = len(items)
    #         if (curr_count > 0):
    #             self.driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", items[-1])
    #         self.driver.execute_script("arguments[0].scrollTop = arguments[0].scrollHeight", list_area)
            
    #         time.sleep(1.5)

    #         logs = self.driver.get_log("performance")
    #         for entry in logs:
    #             try:
    #                 log_json = json.loads(entry["message"])
    #                 message = log_json["message"]
                    
    #                 if (message["method"] == "Network.responseReceived"):
    #                     resp_url = message["params"]["response"]["url"]
                        
    #                     if ("api/articles/complex" in resp_url and "realEstateType" in resp_url):
    #                         request_id = message["params"]["requestId"]
    #                         try:
    #                             response_body = self.driver.execute_cdp_cmd("Network.getResponseBody", {"requestId": request_id})
    #                             data = json.loads(response_body['body'])
    #                             articles = data.get('articleList', [])
                                
    #                             for item in articles:
    #                                 if (item.get("tradeTypeName") != target_type): continue
    #                                 if (item.get("tradeCompleteYN") == "Y"): continue
    #                                 if (item.get("articleStatus") != "R0"): continue
                                    
    #                                 article_no = item.get('articleNo')
    #                                 if (article_no):
    #                                     collected_data_map[article_no] = item
    #                         except:
    #                             pass
    #             except:
    #                 pass
            
    #         if (curr_count == last_count and curr_count > 0):
    #             same_loop += 1
    #             if (same_loop >= 5):
    #                 break
    #         else:
    #             same_loop = 0
            
    #         last_count = curr_count

    #     print(f"   âœ… [{target_type}] 1ì°¨ ìˆ˜ì§‘ ì™„ë£Œ: {len(collected_data_map)}ê±´ (ì¤‘ë³µì œê±°ë¨)")
    #     return collected_data_map

    # def _scroll_and_collect_packets(self, target_type):
    #     print(f"   ğŸ–±ï¸ ìŠ¤í¬ë¡¤ ë° ë°ì´í„° íŒ¨í‚· ìˆ˜ì§‘ ì‹œì‘ ({target_type})")
        
    #     # 1. ëª©ë¡ ì˜ì—­ì´ ë‚˜íƒ€ë‚  ë•Œê¹Œì§€ í™•ì‹¤íˆ ëŒ€ê¸°
    #     try:
    #         wait = WebDriverWait(self.driver, 20)
    #         # articleListAreaê°€ ë©”ëª¨ë¦¬ì— ë¡œë“œë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¼
    #         list_area = wait.until(EC.presence_of_element_located((By.ID, "articleListArea")))
            
    #         # ëª©ë¡ ì˜ì—­ì— í™•ì‹¤íˆ í¬ì»¤ìŠ¤ë¥¼ ì£¼ê¸° ìœ„í•´ JSë¡œ í´ë¦­ ë° ìŠ¤í¬ë¡¤ ìœ„ì¹˜ ì´ˆê¸°í™”
    #         self.driver.execute_script("arguments[0].focus();", list_area)
    #         ActionChains(self.driver).move_to_element(list_area).click().perform()
    #     except Exception as e:
    #         print(f"   âš ï¸ ëª©ë¡ ì˜ì—­ ë¡œë”© ì‹¤íŒ¨: {e}")
    #         # ì˜ì—­ì„ ëª» ì°¾ìœ¼ë©´ ë°”ë””ë¼ë„ ì¡ì§€ë§Œ, ìˆ˜ì§‘ í™•ë¥ ì´ ë‚®ì•„ì§
    #         try:
    #             list_area = self.driver.find_element(By.TAG_NAME, "body")
    #         except:
    #             return {}

    #     collected_data_map = {}
    #     last_count = 0
    #     same_loop = 0
        
    #     for i in range(50): # ìµœëŒ€ 50íšŒ ìŠ¤í¬ë¡¤
    #         # í˜„ì¬ ë¡œë“œëœ ë§¤ë¬¼ ì•„ì´í…œë“¤ í™•ì¸
    #         items = self.driver.find_elements(By.CSS_SELECTOR, "div.item:not(.item--child)")
    #         curr_count = len(items)
            
    #         if curr_count > 0:
    #             # [ê°œì„ ] ë§ˆì§€ë§‰ ì•„ì´í…œìœ¼ë¡œ ìŠ¤í¬ë¡¤í•˜ì—¬ ë‹¤ìŒ ë°ì´í„° ë¡œë”© ìœ ë„
    #             try:
    #                 self.driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", items[-1])
    #             except:
    #                 pass
    #         else:
    #             # ì•„ì´í…œì´ ì•„ì˜ˆ ì—†ìœ¼ë©´ ì˜ì—­ ì „ì²´ë¥¼ ì•„ë˜ë¡œ ê°•ì œ ìŠ¤í¬ë¡¤
    #             self.driver.execute_script("arguments[0].scrollTop = arguments[0].scrollHeight", list_area)
            
    #         # ë„¤ì´ë²„ API ì‘ë‹µ ì‹œê°„ì„ ìœ„í•´ ëŒ€ê¸° ì‹œê°„ì„ 2ì´ˆë¡œ ì†Œí­ ì¦ê°€
    #         time.sleep(2.0)

    #         # --- ë„¤íŠ¸ì›Œí¬ íŒ¨í‚· ë¶„ì„ ë¡œì§ ---
    #         logs = self.driver.get_log("performance")
    #         for entry in logs:
    #             try:
    #                 log_json = json.loads(entry["message"])
    #                 message = log_json["message"]
                    
    #                 if (message["method"] == "Network.responseReceived"):
    #                     resp_url = message["params"]["response"]["url"]
                        
    #                     # ì‹¤ì œ ë§¤ë¬¼ ë°ì´í„° API ì£¼ì†Œì¸ì§€ í™•ì¸
    #                     if ("api/articles/complex" in resp_url and "realEstateType" in resp_url):
    #                         request_id = message["params"]["requestId"]
    #                         try:
    #                             response_body = self.driver.execute_cdp_cmd("Network.getResponseBody", {"requestId": request_id})
    #                             data = json.loads(response_body['body'])
    #                             articles = data.get('articleList', [])
                                
    #                             for item in articles:
    #                                 # í•„í„°ë§ ì¡°ê±´ (íƒ€ì… ì¼ì¹˜, ì™„ë£Œ ì œì™¸, ì •ìƒ ë§¤ë¬¼ë§Œ)
    #                                 if (item.get("tradeTypeName") != target_type): continue
    #                                 if (item.get("tradeCompleteYN") == "Y"): continue
    #                                 if (item.get("articleStatus") != "R0"): continue
                                    
    #                                 article_no = item.get('articleNo')
    #                                 if (article_no):
    #                                     collected_data_map[article_no] = item
    #                         except:
    #                             continue
    #             except:
    #                 continue
            
    #         # ìŠ¤í¬ë¡¤ì„ í•´ë„ ë” ì´ìƒ ë§¤ë¬¼ì´ ëŠ˜ì–´ë‚˜ì§€ ì•Šìœ¼ë©´ ë£¨í”„ íƒˆì¶œ
    #         if (curr_count == last_count and curr_count > 0):
    #             same_loop += 1
    #             if (same_loop >= 5): # 5íšŒ ì—°ì† ë³€í™” ì—†ìœ¼ë©´ ëê¹Œì§€ ì˜¨ ê²ƒìœ¼ë¡œ ê°„ì£¼
    #                 break
    #         else:
    #             same_loop = 0
            
    #         last_count = curr_count
            
    #         # ì§„í–‰ ìƒí™© ì¶œë ¥ (ë„ˆë¬´ ìì£¼ ì°íˆì§€ ì•Šê²Œ 5íšŒë§ˆë‹¤)
    #         if i % 5 == 0:
    #             print(f"   ... ìŠ¤í¬ë¡¤ ì¤‘ ({i}/50), í˜„ì¬ ìˆ˜ì§‘: {len(collected_data_map)}ê±´")

    #     print(f"   âœ… [{target_type}] ìˆ˜ì§‘ ì™„ë£Œ: {len(collected_data_map)}ê±´")
    #     return collected_data_map

    # def _scroll_and_collect_packets(self, target_type):
        print(f"   ğŸ–±ï¸ ìŠ¤í¬ë¡¤ ë° ë°ì´í„° íŒ¨í‚· ìˆ˜ì§‘ ì‹œì‘ ({target_type})")
        
        try:
            list_area = self.driver.find_element(By.ID, "articleListArea")
        except:
            return {}

        collected_data_map = {}
        last_count = 0
        no_change_intervals = 0  # ë°ì´í„°ê°€ ì•ˆ ëŠ˜ì–´ë‚˜ëŠ” íšŸìˆ˜
        
        # 1. í”½ì…€ ë‹¨ìœ„ë¡œ ì¡°ê¸ˆì”© ë‚´ë¦¬ë©° ë¸Œë¼ìš°ì €ê°€ 'ìŠ¤í¬ë¡¤'ì„ ì¸ì‹í•˜ê²Œ í•¨
        scroll_y = 0
        max_scroll_attempts = 60 # ìµœëŒ€ ì‹œë„ íšŸìˆ˜ ëŠ˜ë¦¼

        for i in range(max_scroll_attempts):
            # ì¡°ê¸ˆì”© ì•„ë˜ë¡œ ì´ë™ (íŠ¸ë¦¬ê±° ìœ ë„)
            scroll_y += 1000 
            self.driver.execute_script(f"arguments[0].scrollTop = {scroll_y}", list_area)
            
            # API ì‘ë‹µì´ ì˜¤ê¸°ê¹Œì§€ ì¶©ë¶„íˆ ëŒ€ê¸° (ë§¤ìš° ì¤‘ìš”)
            time.sleep(2.0) 

            # ë„¤íŠ¸ì›Œí¬ ë¡œê·¸ í™•ì¸
            logs = self.driver.get_log("performance")
            for entry in logs:
                try:
                    log_json = json.loads(entry["message"])
                    message = log_json["message"]
                    if message["method"] == "Network.responseReceived":
                        resp_url = message["params"]["response"]["url"]
                        if "api/articles/complex" in resp_url:
                            request_id = message["params"]["requestId"]
                            try:
                                response_body = self.driver.execute_cdp_cmd("Network.getResponseBody", {"requestId": request_id})
                                data = json.loads(response_body['body'])
                                for item in data.get('articleList', []):
                                    if item.get("tradeTypeName") == target_type:
                                        collected_data_map[item.get('articleNo')] = item
                            except: pass
                except: pass

            curr_count = len(collected_data_map)
            
            # ë°ì´í„° ë³€í™” ì²´í¬
            if curr_count > last_count:
                print(f"   ... ë°ì´í„° ìˆ˜ì§‘ ì¤‘: {curr_count}ê±´")
                no_change_intervals = 0 # ë°ì´í„°ê°€ ëŠ˜ì–´ë‚˜ë©´ ì¹´ìš´íŠ¸ ë¦¬ì…‹
            else:
                no_change_intervals += 1
            
            # [í•µì‹¬] ë°ì´í„°ê°€ 20ê±´ ì´ìƒì¸ë°ë„ 5ë²ˆ ì—°ì† ë³€í™”ê°€ ì—†ë‹¤ë©´ ì •ë§ ëì¸ ê²ƒìœ¼ë¡œ ê°„ì£¼
            # í•˜ì§€ë§Œ 20ê±´ ë¯¸ë§Œì´ë¼ë©´(ì²« í˜ì´ì§€ ì‹¤íŒ¨ ìƒí™©) ë” ëˆì§ˆê¸°ê²Œ ê¸°ë‹¤ë¦¼
            if curr_count >= 20 and no_change_intervals >= 5:
                break
            
            # ë§Œì•½ 20ê±´ì—ì„œ ê³„ì† ë©ˆì¶°ìˆë‹¤ë©´ ìŠ¤í¬ë¡¤ ìœ„ì¹˜ë¥¼ ê°•ì œë¡œ ìœ„ì•„ë˜ë¡œ í”ë“¤ì–´ íŠ¸ë¦¬ê±° ì¬ë°œìƒ
            if curr_count == 20 and no_change_intervals == 3:
                self.driver.execute_script(f"arguments[0].scrollTop = {scroll_y - 500}", list_area)
                time.sleep(0.5)

            last_count = curr_count

        return collected_data_map
    
    def collect(self, target_type):
        print(f"\nğŸ” [{target_type}] í”„ë¡œì„¸ìŠ¤ ì‹œì‘...")
        
        print(f"   ğŸŒ í˜ì´ì§€ ì ‘ì†: {COMPLEX_NO}")
        self.driver.get(f"https://new.land.naver.com/complexes/{COMPLEX_NO}")
        self._wait_for_loading()
        
        self._reset_and_apply_filters(target_type)
        
        data_map = self._scroll_and_collect_packets(target_type)
        
        print("   " + "-"*30)
        return data_map

# ==================================================================
# ë©”ì¸ ì‹¤í–‰ ë¸”ë¡ (ì¬ì‹œë„ + ì´ë ¥ ê¸°ë¡ í†µí•©)
# ==================================================================
def main():
    max_retries = 3  # ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
    
    # [ì¤‘ìš”] ì‹œì‘ ì‹œê°„ì„ ê³ ì •í•©ë‹ˆë‹¤. (ì¬ì‹œë„í•˜ë”ë¼ë„ ì²« ì‹œë„ ì‹œê°„ì„ ê¸°ë¡í•´ì•¼ í•¨)
    start_now = datetime.now()
    FIXED_DATE = start_now.strftime("%Y-%m-%d")
    FIXED_TIME = start_now.strftime("%H:%M")
    
    final_status = "FAIL" # ê¸°ë³¸ê°’ì€ ì‹¤íŒ¨ë¡œ ì‹œì‘
    final_count = 0
    last_error_msg = ""
    
    print(f"\nğŸ•’ ì‘ì—… ê¸°ì¤€ ì‹œê°„: {FIXED_DATE} {FIXED_TIME}")

    for attempt in range(max_retries):
        crawler = None 
        try:
            print(f"\nğŸš€ í¬ë¡¤ë§ ì‹œë„ ({attempt + 1}/{max_retries})")
            
            # --- ì—¬ê¸°ì„œ ì—ëŸ¬ê°€ ë‚˜ë©´ exceptë¡œ ì í”„í•©ë‹ˆë‹¤ ---
            crawler = NaverLandCrawler()
            
            # 1. í¬ë¡¤ë§ ìˆ˜í–‰
            sale_map = crawler.collect("ë§¤ë§¤")
            jeonse_map = crawler.collect("ì „ì„¸")
            
            print(f"   ğŸ“Š ìˆ˜ì§‘ ê²°ê³¼: ë§¤ë§¤ {len(sale_map)}ê±´, ì „ì„¸ {len(jeonse_map)}ê±´")
            
            # 2. ë°ì´í„° ì •ì œ (ê³ ì •ëœ ì‹œê°„ FIXED_TIME ì‚¬ìš©)
            clean_sale = refine_data(list(sale_map.values()), "ë§¤ë§¤", FIXED_DATE, FIXED_TIME)
            clean_jeonse = refine_data(list(jeonse_map.values()), "ì „ì„¸", FIXED_DATE, FIXED_TIME)
            
            # 3. ë°ì´í„° í†µí•©
            final_db_data = clean_sale + clean_jeonse
            final_count = len(final_db_data)
            
            # 4. DB ì €ì¥
            if final_db_data:
                print(f"ğŸ’¾ ì´ {final_count}ê±´ì˜ ë°ì´í„°ë¥¼ DBì— ì €ì¥í•©ë‹ˆë‹¤...")
                save_to_supabase(final_db_data)
            else:
                print("âš ï¸ ì €ì¥í•  ë°ì´í„°ê°€ 0ê±´ì…ë‹ˆë‹¤.")

            # ì—¬ê¸°ê¹Œì§€ ì˜¤ë©´ ì„±ê³µ
            final_status = "SUCCESS"
            last_error_msg = "" # ì„±ê³µ ì‹œ ì—ëŸ¬ ë©”ì‹œì§€ ì´ˆê¸°í™”
            
            print("âœ¨ í¬ë¡¤ë§ ë° ì €ì¥ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            break # ì„±ê³µí–ˆìœ¼ë‹ˆ ë£¨í”„ íƒˆì¶œ

        except Exception as e:
            print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ (ì‹œë„ {attempt + 1}): {e}")
            last_error_msg = str(e) # ì—ëŸ¬ ë©”ì‹œì§€ ë³´ê´€
            
            # ë¸Œë¼ìš°ì € ì •ë¦¬
            if crawler:
                try: crawler.close()
                except: pass

            # ë§ˆì§€ë§‰ ì‹œë„ê°€ ì•„ë‹ˆë©´ ëŒ€ê¸° í›„ ì¬ì‹œë„
            if attempt < max_retries - 1:
                print("ğŸ”„ 10ì´ˆ í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤...")
                time.sleep(10)
            else:
                print("ğŸ’€ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.")

    # [í•µì‹¬] ì„±ê³µ/ì‹¤íŒ¨ ì—¬ë¶€ì— ìƒê´€ì—†ì´ ì´ë ¥ì„ ê¸°ë¡í•¨
    print("\n" + "="*50)
    save_crawl_history(FIXED_DATE, FIXED_TIME, final_status, final_count, last_error_msg)
    print("="*50)

    # ë§ˆì§€ë§‰ìœ¼ë¡œ ë¸Œë¼ìš°ì € ì •ë¦¬
    if crawler:
        try: crawler.close()
        except: pass

    # ìµœì¢… ìƒíƒœê°€ FAILì´ë©´ ì‹œìŠ¤í…œ ì¢…ë£Œ ì½”ë“œ 1 ë°˜í™˜ (Crontab ë“±ì—ì„œ ì—ëŸ¬ ì¸ì‹ìš©)
    if final_status == "FAIL":
        sys.exit(1)

if __name__ == "__main__":
    main()