import os
import json
import time
import re
from datetime import datetime, timedelta, timezone
from pathlib import Path

import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.common.action_chains import ActionChains
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

from dotenv import load_dotenv
from supabase import create_client, Client

# ==================================================================
# [ì„¤ì •] í™˜ê²½ë³€ìˆ˜ ë° ìƒìˆ˜ ì •ì˜
# ==================================================================
COMPLEX_NO = "108064"
KST = timezone(timedelta(hours=9))

# 1. í˜„ì¬ íŒŒì¼ ìœ„ì¹˜ ê¸°ì¤€ .env.local ë¡œë“œ
current_dir = os.path.dirname(os.path.abspath(__file__))
env_path = os.path.join(current_dir, 'land-dashboard/.env.local') # ê²½ë¡œ í™•ì¸ í•„ìš”

load_result = load_dotenv(dotenv_path=env_path)
print(f"ğŸ“‚ ê²½ë¡œ: {env_path}")
print(f"ğŸ”„ ë¡œë“œ ê²°ê³¼: {load_result}")

SUPABASE_URL = os.environ.get("NEXT_PUBLIC_SUPABASE_URL")
SUPABASE_KEY = os.environ.get("NEXT_PUBLIC_SUPABASE_KEY")

if SUPABASE_URL:
    print(f"âœ… URL ë¡œë“œ ì„±ê³µ: {SUPABASE_URL[:10]}...")
else:
    print("âŒ URL ë¡œë“œ ì‹¤íŒ¨ (DB ì €ì¥ ë¶ˆê°€)")


# ==================================================================
# [í•¨ìˆ˜] ë°ì´í„° ì •ì œ ë° DB ì €ì¥
# ==================================================================
def refine_data(raw_data_list, trade_type, fixed_date, fixed_time):
    """
    ë„¤ì´ë²„ ì›ë³¸ ë°ì´í„° ë¦¬ìŠ¤íŠ¸ë¥¼ DB ìŠ¤í‚¤ë§ˆì— ë§ê²Œ ë³€í™˜
    """
    refined_list = []
    
    now = datetime.now()
    today_str = now.strftime("%Y-%m-%d")
    hour_str = f"{now.strftime('%H')}ì‹œ"

    for item in raw_data_list:

        
        # ë„¤ì´ë²„ API ì‘ë‹µ í‚¤(Key) ë§¤í•‘
        # (ì‹¤ì œ ì‘ë‹µì— ë”°ë¼ í‚¤ ì´ë¦„ì€ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìœ¼ë‹ˆ .getìœ¼ë¡œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬)
        
        
        # ê°€ê²© ì •ë³´ (dealOrWarrantPrc: "15ì–µ 5,000" í˜•íƒœ)
        price_str = item.get('dealOrWarrantPrc', '')
        
        area_name = item.get('areaName', '')   # 110E-2
        area_ex = item.get('area2', '')        # 84 (ì „ìš©ë©´ì )
        floor = item.get('floorInfo', '')      # ì €/22ì¸µ (API í‚¤ëŠ” ë³´í†µ floorInfo ì…ë‹ˆë‹¤)
        direction = item.get('direction', '')  # ë‚¨ì„œí–¥
        formatted_spec = f"{area_name}/{area_ex}mÂ², {floor}, {direction}"

        refined_item = {
             "crawl_date": fixed_date,
             "crawl_time": fixed_time,
             "article_no": item.get('articleNo', ''),  # ë§¤ë¬¼ ë²ˆí˜¸ (PK)
             "trade_type": trade_type,                 # ë§¤ë§¤/ì „ì„¸
             "price": price_str,                       # ê°€ê²© (ë¬¸ìì—´ ê·¸ëŒ€ë¡œ ì €ì¥)
             "dong": item.get('buildingName'),         # ë™
             "spec": formatted_spec,
             "agent": item.get('realtorName'),   # ì¤‘ê°œì—…ì†Œ
             "provider": item.get('cpName'),            # ì œê³µ ì—…ì²´(ex. ë§¤ê²½ ë¶€ë™ì‚°, ì•„ì‹¤ ë“±)
             "confirm_date": item.get('articleConfirmYmd',''), # í™•ì¸ë‚ ì§œ
             "is_owner": item.get('verificationTypeCode') == 'OWNER' # ì§‘ì£¼ì¸ ì¸ì¦ì—¬ë¶€
        }
        # refined_item = {
         #   "article_no": item.get('articleNo', ''),                # ë§¤ë¬¼ ë²ˆí˜¸ (PK)
          #  "trade_type": trade_type,                               # ë§¤ë§¤/ì „ì„¸
           # "price": price_str,                                     # ê°€ê²© (ë¬¸ìì—´ ê·¸ëŒ€ë¡œ ì €ì¥)
           # "dong": item.get('dongName', ''),                       # ë™ ì´ë¦„
        #   "floor": item.get('floorInfo', ''),                     # ì¸µìˆ˜ (ì˜ˆ: 5/15)
            # "spec": item.get('areaName', ''),                       # ë©´ì  (ì˜ˆ: 84A)
            # "direction": item.get('direction', ''),                 # í–¥ (ë‚¨í–¥ ë“±)
            # "agent": item.get('realtorName', item.get('cpName', '')), # ì¤‘ê°œì‚¬ëª…
            # "description": item.get('articleFeatureDesc', ''),      # íŠ¹ì§• ì„¤ëª…
            # "is_landlord": True if item.get('directTradYn') == 'Y' else False, # ì§ê±°ë˜/ì§‘ì£¼ì¸ ì—¬ë¶€
            # "verification_date": item.get('articleConfirmYmd', ''), # í™•ì¸ ì¼ì
            # "crawl_date": today_str,
            # "crawl_time": hour_str
        # }
        refined_list.append(refined_item)
    
    return refined_list

def save_to_supabase(data_list):
    """
    Supabase DBì— ë°ì´í„° ì €ì¥ (Upsert)
    """
    if not data_list or not SUPABASE_URL:
        print("âš ï¸ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ê±°ë‚˜ DB ì„¤ì •ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return

    try:
        supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
        table_name = "real_estate_logs"  # âš ï¸ ì‹¤ì œ ì‚¬ìš©í•˜ëŠ” í…Œì´ë¸”ëª…ìœ¼ë¡œ ë³€ê²½ í•„ìˆ˜!

        # upsert: article_no(PK)ê°€ ê°™ìœ¼ë©´ ì—…ë°ì´íŠ¸, ì—†ìœ¼ë©´ ì¶”ê°€
        response = supabase.table(table_name).upsert(data_list).execute()
        
        print(f"âœ… DB ì €ì¥ ì™„ë£Œ! (ì´ {len(data_list)}ê±´ ì²˜ë¦¬)")
        
    except Exception as e:
        print(f"âŒ DB ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


# ==================================================================
# [í´ë˜ìŠ¤] í¬ë¡¤ëŸ¬ ì •ì˜
# ==================================================================
class NaverLandCrawler:
    
    def __init__(self):
        """ìƒì„±ì: ë“œë¼ì´ë²„ ì´ˆê¸°í™”"""
        self.driver = self._init_driver()

    def _init_driver(self):
        """ë“œë¼ì´ë²„ ì˜µì…˜ ì„¤ì •"""
        options = uc.ChromeOptions()
        options.add_argument("--headless=new") # í…ŒìŠ¤íŠ¸í•  ë• ì£¼ì„ ì²˜ë¦¬ ì¶”ì²œ (í™”ë©´ ë³´ê²Œ)
        options.add_argument("--window-size=1920,1080")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--disable-gpu")
        options.add_argument("--lang=ko_KR")
        options.add_argument("--disable-blink-features=AutomationControlled")
        
        prefs = {"profile.managed_default_content_settings.images": 2}
        options.add_experimental_option("prefs", prefs)
        options.set_capability("goog:loggingPrefs", {"performance": "ALL"})

        # ì •ì‹ ë²„ì „ ì‚¬ìš© ê¶Œì¥ (ë²„ì „ ëª…ì‹œ)
        driver = uc.Chrome(options=options, version_main=142) 
        
        ua = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        driver.execute_cdp_cmd("Network.setUserAgentOverride", {
            "userAgent": ua,
            "acceptLanguage": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
            "platform": "MacIntel"
        })
        driver.execute_cdp_cmd("Network.setExtraHTTPHeaders", {
            "headers": {
                "Referer": "https://new.land.naver.com/",
                "Origin": "https://new.land.naver.com"
            }
        })
        
        return driver

    def close(self):
        if (self.driver):
            print("\nğŸ‘‹ í¬ë¡¤ëŸ¬ ì¢…ë£Œ (ë¸Œë¼ìš°ì € ë‹«ê¸°)")
            self.driver.quit()

    def _wait_for_loading(self):
        try:
            WebDriverWait(self.driver, 20).until(EC.presence_of_element_located((By.ID, "articleListArea")))
        except Exception as e:
            print(f"   âš ï¸ ë¡œë”© ëŒ€ê¸° ì‹¤íŒ¨: {e}")

    def _reset_and_apply_filters(self, target_type):
        print(f"   âš™ï¸ í•„í„° ì ìš© ì¤‘: {target_type}")
        
        # 1. ì „ì²´ ê±°ë˜ë°©ì‹ í•´ì œ
        self.driver.execute_script("if(document.querySelector('#complex_article_trad_type_filter_0:checked')) { document.querySelector('#complex_article_trad_type_filter_0').click(); }")
        time.sleep(0.5)

        # 2. íƒ€ê²Ÿ íƒ€ì… ì„¤ì •
        if (target_type == "ë§¤ë§¤"):
            self.driver.execute_script("if(!document.querySelector('#complex_article_trad_type_filter_1:checked')) { document.querySelector('#complex_article_trad_type_filter_1').click(); }")
            self.driver.execute_script("if(document.querySelector('#complex_article_trad_type_filter_2:checked')) { document.querySelector('#complex_article_trad_type_filter_2').click(); }")
        
        elif (target_type == "ì „ì„¸"):
            self.driver.execute_script("if(document.querySelector('#complex_article_trad_type_filter_1:checked')) { document.querySelector('#complex_article_trad_type_filter_1').click(); }")
            self.driver.execute_script("if(!document.querySelector('#complex_article_trad_type_filter_2:checked')) { document.querySelector('#complex_article_trad_type_filter_2').click(); }")

        time.sleep(1)

        # 3. ë¬¶ê¸° í•´ì œ
        try:
            group_chk = self.driver.find_element(By.ID, "address_group2")
            if (group_chk.is_selected()):
                self.driver.execute_script("arguments[0].click();", self.driver.find_element(By.CSS_SELECTOR, "label[for='address_group2']"))
        except:
            pass

        # 4. ê°€ê²©ìˆœ ì •ë ¬
        try:
            self.driver.find_element(By.CSS_SELECTOR, "a.sorting_type[data-nclk='TAA.price']").click()
        except:
            pass
        
        time.sleep(3)

    def _scroll_and_collect_packets(self, target_type):
        try:
            list_area = self.driver.find_element(By.ID, "articleListArea")
        except:
            list_area = self.driver.find_element(By.TAG_NAME, "body")
            
        try:
            ActionChains(self.driver).move_to_element(list_area).click().perform()
        except:
            pass

        collected_data_map = {}
        last_count = 0
        same_loop = 0
        
        for i in range(50): # ìµœëŒ€ 50íšŒ ìŠ¤í¬ë¡¤
            items = self.driver.find_elements(By.CSS_SELECTOR, "div.item:not(.item--child)")
            curr_count = len(items)
            if (curr_count > 0):
                self.driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", items[-1])
            self.driver.execute_script("arguments[0].scrollTop = arguments[0].scrollHeight", list_area)
            
            time.sleep(1.5)

            logs = self.driver.get_log("performance")
            for entry in logs:
                try:
                    log_json = json.loads(entry["message"])
                    message = log_json["message"]
                    
                    if (message["method"] == "Network.responseReceived"):
                        resp_url = message["params"]["response"]["url"]
                        
                        if ("api/articles/complex" in resp_url and "realEstateType" in resp_url):
                            request_id = message["params"]["requestId"]
                            try:
                                response_body = self.driver.execute_cdp_cmd("Network.getResponseBody", {"requestId": request_id})
                                data = json.loads(response_body['body'])
                                articles = data.get('articleList', [])
                                
                                for item in articles:
                                    if (item.get("tradeTypeName") != target_type): continue
                                    if (item.get("tradeCompleteYN") == "Y"): continue
                                    if (item.get("articleStatus") != "R0"): continue
                                    
                                    article_no = item.get('articleNo')
                                    if (article_no):
                                        collected_data_map[article_no] = item
                            except:
                                pass
                except:
                    pass
            
            if (curr_count == last_count and curr_count > 0):
                same_loop += 1
                if (same_loop >= 5):
                    break
            else:
                same_loop = 0
            
            last_count = curr_count

        print(f"   âœ… [{target_type}] 1ì°¨ ìˆ˜ì§‘ ì™„ë£Œ: {len(collected_data_map)}ê±´ (ì¤‘ë³µì œê±°ë¨)")
        return collected_data_map

    def collect(self, target_type):
        print(f"\nğŸ” [{target_type}] í”„ë¡œì„¸ìŠ¤ ì‹œì‘...")
        
        print(f"   ğŸŒ í˜ì´ì§€ ì ‘ì†: {COMPLEX_NO}")
        self.driver.get(f"https://new.land.naver.com/complexes/{COMPLEX_NO}")
        self._wait_for_loading()
        
        self._reset_and_apply_filters(target_type)
        
        data_map = self._scroll_and_collect_packets(target_type)
        
        print("   " + "-"*30)
        return data_map

# ==================================================================
# ë©”ì¸ ì‹¤í–‰ ë¸”ë¡
# ==================================================================
def main():
    crawler = NaverLandCrawler()
    
    start_now = datetime.now()
    FIXED_DATE = start_now.strftime("%Y-%m-%d")
    FIXED_TIME = start_now.strftime("%H:%M") # ë¶„ ë‹¨ìœ„ê¹Œì§€ ê¸°ë¡ (ì˜ˆ: 14:00, 14:20)
    
    try:
        # 1. í¬ë¡¤ë§ ìˆ˜í–‰ (Map í˜•íƒœë¡œ ë°˜í™˜ë¨)
        sale_map = crawler.collect("ë§¤ë§¤")
        jeonse_map = crawler.collect("ì „ì„¸")
        
        print("\n" + "="*60)
        print(f"ğŸ“ ìˆ˜ì§‘ ê²°ê³¼: ë§¤ë§¤ {len(sale_map)}ê±´, ì „ì„¸ {len(jeonse_map)}ê±´")
        
        # 2. ë°ì´í„° ì •ì œ (Map -> List ë³€í™˜ í›„ í•¨ìˆ˜ í˜¸ì¶œ)
        # .values()ë¥¼ ì‚¬ìš©í•˜ì—¬ ë”•ì…”ë„ˆë¦¬ì˜ ê°’(ë°ì´í„° ê°ì²´)ë“¤ë§Œ ë¦¬ìŠ¤íŠ¸ë¡œ ë½‘ì•„ëƒ…ë‹ˆë‹¤.
        clean_sale = refine_data(list(sale_map.values()), "ë§¤ë§¤", FIXED_DATE, FIXED_TIME)
        clean_jeonse = refine_data(list(jeonse_map.values()), "ì „ì„¸",FIXED_DATE, FIXED_TIME)
        
        # 3. ë°ì´í„° í†µí•©
        final_db_data = clean_sale + clean_jeonse
        
        # 4. DB ì €ì¥
        if final_db_data:
            print(f"ğŸ’¾ ì´ {len(final_db_data)}ê±´ì˜ ë°ì´í„°ë¥¼ DBì— ì €ì¥í•©ë‹ˆë‹¤...")
            save_to_supabase(final_db_data)
        else:
            print("âš ï¸ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

        print("="*60)

    except Exception as e:
        print(f"âŒ ë©”ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
    finally:
        crawler.close()

if __name__ == "__main__":
    main()